{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGWNn+/Mk6eVcdm879VdUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hakieh/-/blob/main/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLmGt7zWfAoh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC4_xTomhkBl",
        "outputId": "f7c38f99-7e9c-498f-8981-71632c0bb597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DevilExileSu/transformer.git\n"
      ],
      "metadata": {
        "id": "87xffjSBfC9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82d01a15-a209-4a5c-cd51-ff48f7dbf182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformer' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd transformer/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTT7K8GUggVv",
        "outputId": "80154c52-63fe-4f90-b8f6-d2abb64a6396"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --batch_size 64 --h_dim 256 --lr 0.001 --epochs 50"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4crrpE5gv1d",
        "outputId": "3b520d2a-aa8c-4c28-c057-4f81849160a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-14 02:54:07,510 [DEBUG]: \n",
            "Running with the following configs:\n",
            "\tsrc_train_data : dataset/train.zh.token\n",
            "\ttrg_train_data : dataset/train.en.token\n",
            "\tsrc_valid_data : dataset/val.zh.token\n",
            "\ttrg_valid_data : dataset/val.en.token\n",
            "\tsrc_vocab : dataset/zh_vocab.pkl\n",
            "\ttrg_vocab : dataset/en_vocab.pkl\n",
            "\tshuffle : True\n",
            "\tbatch_size : 64\n",
            "\th_dim : 256\n",
            "\tenc_n_layers : 3\n",
            "\tdec_n_layers : 3\n",
            "\tenc_n_heads : 8\n",
            "\tdec_n_heads : 8\n",
            "\tenc_dropout : 0.1\n",
            "\tdec_dropout : 0.1\n",
            "\tenc_pf_dim : 512\n",
            "\tdec_pf_dim : 512\n",
            "\tlr : 0.001\n",
            "\toptimizer : adam\n",
            "\tl2 : 0.0\n",
            "\tepochs : 50\n",
            "\tsave_dir : ./saved_models\n",
            "\tsave_epochs : 5\n",
            "\tearly_stop : True\n",
            "\tpatience : 10\n",
            "\tresume : False\n",
            "\tresume_path : ./saved_models/model_best.pt\n",
            "\tlog_step : 20\n",
            "\tcuda : True\n",
            "\tconfig_file : ./config.json\n",
            "\tseed : 1234\n",
            "\n",
            "\n",
            "DEBUG:10:\n",
            "Running with the following configs:\n",
            "\tsrc_train_data : dataset/train.zh.token\n",
            "\ttrg_train_data : dataset/train.en.token\n",
            "\tsrc_valid_data : dataset/val.zh.token\n",
            "\ttrg_valid_data : dataset/val.en.token\n",
            "\tsrc_vocab : dataset/zh_vocab.pkl\n",
            "\ttrg_vocab : dataset/en_vocab.pkl\n",
            "\tshuffle : True\n",
            "\tbatch_size : 64\n",
            "\th_dim : 256\n",
            "\tenc_n_layers : 3\n",
            "\tdec_n_layers : 3\n",
            "\tenc_n_heads : 8\n",
            "\tdec_n_heads : 8\n",
            "\tenc_dropout : 0.1\n",
            "\tdec_dropout : 0.1\n",
            "\tenc_pf_dim : 512\n",
            "\tdec_pf_dim : 512\n",
            "\tlr : 0.001\n",
            "\toptimizer : adam\n",
            "\tl2 : 0.0\n",
            "\tepochs : 50\n",
            "\tsave_dir : ./saved_models\n",
            "\tsave_epochs : 5\n",
            "\tearly_stop : True\n",
            "\tpatience : 10\n",
            "\tresume : False\n",
            "\tresume_path : ./saved_models/model_best.pt\n",
            "\tlog_step : 20\n",
            "\tcuda : True\n",
            "\tconfig_file : ./config.json\n",
            "\tseed : 1234\n",
            "\n",
            "\n",
            "2022-12-14 02:54:07,511 [DEBUG]: Config saved to file ./config.json\n",
            "DEBUG:10:Config saved to file ./config.json\n",
            "2022-12-14 02:54:07,586 [DEBUG]: -----------read data-----------\n",
            "DEBUG:10:-----------read data-----------\n",
            "2022-12-14 02:54:09,074 [DEBUG]: dataset/train.zh.token and dataset/train.en.token has data 169337\n",
            "DEBUG:10:dataset/train.zh.token and dataset/train.en.token has data 169337\n",
            "2022-12-14 02:54:09,074 [DEBUG]: -----------preprocess data-----------\n",
            "DEBUG:10:-----------preprocess data-----------\n",
            "2022-12-14 02:54:11,268 [DEBUG]: -----------read data-----------\n",
            "DEBUG:10:-----------read data-----------\n",
            "2022-12-14 02:54:11,282 [DEBUG]: dataset/val.zh.token and dataset/val.en.token has data 8000\n",
            "DEBUG:10:dataset/val.zh.token and dataset/val.en.token has data 8000\n",
            "2022-12-14 02:54:11,282 [DEBUG]: -----------preprocess data-----------\n",
            "DEBUG:10:-----------preprocess data-----------\n",
            "2022-12-14 02:54:12,029 [INFO]: Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (word_embeddings): WordEmbeddings(41694, 256)\n",
            "    (pe): PositionEmbeddings(200, 256)\n",
            "    (layers): ModuleList(\n",
            "      (0): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (word_embeddings): WordEmbeddings(33222, 256)\n",
            "    (pe): PositionEmbeddings(200, 256)\n",
            "    (layers): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=33222, bias=True)\n",
            ")\n",
            "Trainable parameters: 31772614\n",
            "INFO:20:Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (word_embeddings): WordEmbeddings(41694, 256)\n",
            "    (pe): PositionEmbeddings(200, 256)\n",
            "    (layers): ModuleList(\n",
            "      (0): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (word_embeddings): WordEmbeddings(33222, 256)\n",
            "    (pe): PositionEmbeddings(200, 256)\n",
            "    (layers): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (attention): MultiHeadAttentionLayer(\n",
            "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (positionwise_feedforward): PositionwiseFeedforwardLayer(\n",
            "          (fc_1): Linear(in_features=256, out_features=512, bias=True)\n",
            "          (fc_2): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (self_attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (attention_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (self_attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (attention_dropout): Dropout(p=0.1, inplace=False)\n",
            "        (ff_dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=33222, bias=True)\n",
            ")\n",
            "Trainable parameters: 31772614\n",
            "2022-12-14 02:54:12,063 [DEBUG]: 5\n",
            "DEBUG:10:5\n",
            "2022-12-14 02:54:12,063 [DEBUG]: #######: 1 ...\n",
            "DEBUG:10:#######: 1 ...\n",
            "2022-12-14 02:54:12,916 [INFO]: Train Epoch: 1, 0/2646 (0%), Loss: 10.404480\n",
            "INFO:20:Train Epoch: 1, 0/2646 (0%), Loss: 10.404480\n",
            "2022-12-14 02:54:15,564 [INFO]: Train Epoch: 1, 20/2646 (1%), Loss: 6.917541\n",
            "INFO:20:Train Epoch: 1, 20/2646 (1%), Loss: 6.917541\n",
            "2022-12-14 02:54:18,289 [INFO]: Train Epoch: 1, 40/2646 (2%), Loss: 6.899717\n",
            "INFO:20:Train Epoch: 1, 40/2646 (2%), Loss: 6.899717\n",
            "2022-12-14 02:54:20,976 [INFO]: Train Epoch: 1, 60/2646 (2%), Loss: 6.738795\n",
            "INFO:20:Train Epoch: 1, 60/2646 (2%), Loss: 6.738795\n",
            "2022-12-14 02:54:23,778 [INFO]: Train Epoch: 1, 80/2646 (3%), Loss: 6.927283\n",
            "INFO:20:Train Epoch: 1, 80/2646 (3%), Loss: 6.927283\n",
            "2022-12-14 02:54:26,513 [INFO]: Train Epoch: 1, 100/2646 (4%), Loss: 6.885287\n",
            "INFO:20:Train Epoch: 1, 100/2646 (4%), Loss: 6.885287\n",
            "2022-12-14 02:54:29,314 [INFO]: Train Epoch: 1, 120/2646 (5%), Loss: 6.966648\n",
            "INFO:20:Train Epoch: 1, 120/2646 (5%), Loss: 6.966648\n",
            "2022-12-14 02:54:32,024 [INFO]: Train Epoch: 1, 140/2646 (5%), Loss: 6.902804\n",
            "INFO:20:Train Epoch: 1, 140/2646 (5%), Loss: 6.902804\n",
            "2022-12-14 02:54:34,834 [INFO]: Train Epoch: 1, 160/2646 (6%), Loss: 6.679322\n",
            "INFO:20:Train Epoch: 1, 160/2646 (6%), Loss: 6.679322\n",
            "2022-12-14 02:54:37,526 [INFO]: Train Epoch: 1, 180/2646 (7%), Loss: 6.633649\n",
            "INFO:20:Train Epoch: 1, 180/2646 (7%), Loss: 6.633649\n",
            "2022-12-14 02:54:40,393 [INFO]: Train Epoch: 1, 200/2646 (8%), Loss: 6.675128\n",
            "INFO:20:Train Epoch: 1, 200/2646 (8%), Loss: 6.675128\n",
            "2022-12-14 02:54:43,582 [INFO]: Train Epoch: 1, 220/2646 (8%), Loss: 6.682809\n",
            "INFO:20:Train Epoch: 1, 220/2646 (8%), Loss: 6.682809\n",
            "2022-12-14 02:54:46,494 [INFO]: Train Epoch: 1, 240/2646 (9%), Loss: 6.680223\n",
            "INFO:20:Train Epoch: 1, 240/2646 (9%), Loss: 6.680223\n",
            "2022-12-14 02:54:49,282 [INFO]: Train Epoch: 1, 260/2646 (10%), Loss: 6.656691\n",
            "INFO:20:Train Epoch: 1, 260/2646 (10%), Loss: 6.656691\n",
            "2022-12-14 02:54:52,060 [INFO]: Train Epoch: 1, 280/2646 (11%), Loss: 6.633048\n",
            "INFO:20:Train Epoch: 1, 280/2646 (11%), Loss: 6.633048\n",
            "2022-12-14 02:54:54,860 [INFO]: Train Epoch: 1, 300/2646 (11%), Loss: 6.658467\n",
            "INFO:20:Train Epoch: 1, 300/2646 (11%), Loss: 6.658467\n",
            "2022-12-14 02:54:57,619 [INFO]: Train Epoch: 1, 320/2646 (12%), Loss: 6.790578\n",
            "INFO:20:Train Epoch: 1, 320/2646 (12%), Loss: 6.790578\n",
            "2022-12-14 02:55:00,449 [INFO]: Train Epoch: 1, 340/2646 (13%), Loss: 6.760048\n",
            "INFO:20:Train Epoch: 1, 340/2646 (13%), Loss: 6.760048\n",
            "2022-12-14 02:55:03,486 [INFO]: Train Epoch: 1, 360/2646 (14%), Loss: 6.588280\n",
            "INFO:20:Train Epoch: 1, 360/2646 (14%), Loss: 6.588280\n",
            "2022-12-14 02:55:06,329 [INFO]: Train Epoch: 1, 380/2646 (14%), Loss: 6.609067\n",
            "INFO:20:Train Epoch: 1, 380/2646 (14%), Loss: 6.609067\n",
            "2022-12-14 02:55:09,213 [INFO]: Train Epoch: 1, 400/2646 (15%), Loss: 6.752658\n",
            "INFO:20:Train Epoch: 1, 400/2646 (15%), Loss: 6.752658\n",
            "2022-12-14 02:55:12,099 [INFO]: Train Epoch: 1, 420/2646 (16%), Loss: 6.465266\n",
            "INFO:20:Train Epoch: 1, 420/2646 (16%), Loss: 6.465266\n",
            "2022-12-14 02:55:14,946 [INFO]: Train Epoch: 1, 440/2646 (17%), Loss: 6.636740\n",
            "INFO:20:Train Epoch: 1, 440/2646 (17%), Loss: 6.636740\n",
            "2022-12-14 02:55:17,680 [INFO]: Train Epoch: 1, 460/2646 (17%), Loss: 6.688357\n",
            "INFO:20:Train Epoch: 1, 460/2646 (17%), Loss: 6.688357\n",
            "2022-12-14 02:55:20,863 [INFO]: Train Epoch: 1, 480/2646 (18%), Loss: 6.672470\n",
            "INFO:20:Train Epoch: 1, 480/2646 (18%), Loss: 6.672470\n",
            "2022-12-14 02:55:23,748 [INFO]: Train Epoch: 1, 500/2646 (19%), Loss: 6.571148\n",
            "INFO:20:Train Epoch: 1, 500/2646 (19%), Loss: 6.571148\n",
            "2022-12-14 02:55:26,806 [INFO]: Train Epoch: 1, 520/2646 (20%), Loss: 6.551127\n",
            "INFO:20:Train Epoch: 1, 520/2646 (20%), Loss: 6.551127\n",
            "2022-12-14 02:55:29,748 [INFO]: Train Epoch: 1, 540/2646 (20%), Loss: 6.630628\n",
            "INFO:20:Train Epoch: 1, 540/2646 (20%), Loss: 6.630628\n",
            "2022-12-14 02:55:32,687 [INFO]: Train Epoch: 1, 560/2646 (21%), Loss: 6.635487\n",
            "INFO:20:Train Epoch: 1, 560/2646 (21%), Loss: 6.635487\n",
            "2022-12-14 02:55:35,661 [INFO]: Train Epoch: 1, 580/2646 (22%), Loss: 6.639841\n",
            "INFO:20:Train Epoch: 1, 580/2646 (22%), Loss: 6.639841\n",
            "2022-12-14 02:55:38,467 [INFO]: Train Epoch: 1, 600/2646 (23%), Loss: 6.532496\n",
            "INFO:20:Train Epoch: 1, 600/2646 (23%), Loss: 6.532496\n",
            "2022-12-14 02:55:41,310 [INFO]: Train Epoch: 1, 620/2646 (23%), Loss: 6.564268\n",
            "INFO:20:Train Epoch: 1, 620/2646 (23%), Loss: 6.564268\n",
            "2022-12-14 02:55:44,069 [INFO]: Train Epoch: 1, 640/2646 (24%), Loss: 6.499243\n",
            "INFO:20:Train Epoch: 1, 640/2646 (24%), Loss: 6.499243\n",
            "2022-12-14 02:55:46,896 [INFO]: Train Epoch: 1, 660/2646 (25%), Loss: 6.567498\n",
            "INFO:20:Train Epoch: 1, 660/2646 (25%), Loss: 6.567498\n",
            "2022-12-14 02:55:49,658 [INFO]: Train Epoch: 1, 680/2646 (26%), Loss: 6.657373\n",
            "INFO:20:Train Epoch: 1, 680/2646 (26%), Loss: 6.657373\n",
            "2022-12-14 02:55:52,404 [INFO]: Train Epoch: 1, 700/2646 (26%), Loss: 6.561598\n",
            "INFO:20:Train Epoch: 1, 700/2646 (26%), Loss: 6.561598\n",
            "2022-12-14 02:55:55,077 [INFO]: Train Epoch: 1, 720/2646 (27%), Loss: 6.636931\n",
            "INFO:20:Train Epoch: 1, 720/2646 (27%), Loss: 6.636931\n",
            "2022-12-14 02:55:57,912 [INFO]: Train Epoch: 1, 740/2646 (28%), Loss: 6.526362\n",
            "INFO:20:Train Epoch: 1, 740/2646 (28%), Loss: 6.526362\n",
            "2022-12-14 02:56:00,734 [INFO]: Train Epoch: 1, 760/2646 (29%), Loss: 6.664277\n",
            "INFO:20:Train Epoch: 1, 760/2646 (29%), Loss: 6.664277\n",
            "2022-12-14 02:56:03,556 [INFO]: Train Epoch: 1, 780/2646 (29%), Loss: 6.647657\n",
            "INFO:20:Train Epoch: 1, 780/2646 (29%), Loss: 6.647657\n",
            "2022-12-14 02:56:06,512 [INFO]: Train Epoch: 1, 800/2646 (30%), Loss: 6.617333\n",
            "INFO:20:Train Epoch: 1, 800/2646 (30%), Loss: 6.617333\n",
            "2022-12-14 02:56:09,351 [INFO]: Train Epoch: 1, 820/2646 (31%), Loss: 6.640784\n",
            "INFO:20:Train Epoch: 1, 820/2646 (31%), Loss: 6.640784\n",
            "2022-12-14 02:56:12,085 [INFO]: Train Epoch: 1, 840/2646 (32%), Loss: 6.578098\n",
            "INFO:20:Train Epoch: 1, 840/2646 (32%), Loss: 6.578098\n",
            "2022-12-14 02:56:14,864 [INFO]: Train Epoch: 1, 860/2646 (33%), Loss: 6.504827\n",
            "INFO:20:Train Epoch: 1, 860/2646 (33%), Loss: 6.504827\n",
            "2022-12-14 02:56:17,683 [INFO]: Train Epoch: 1, 880/2646 (33%), Loss: 6.680529\n",
            "INFO:20:Train Epoch: 1, 880/2646 (33%), Loss: 6.680529\n",
            "2022-12-14 02:56:20,545 [INFO]: Train Epoch: 1, 900/2646 (34%), Loss: 6.572914\n",
            "INFO:20:Train Epoch: 1, 900/2646 (34%), Loss: 6.572914\n",
            "2022-12-14 02:56:23,346 [INFO]: Train Epoch: 1, 920/2646 (35%), Loss: 6.628529\n",
            "INFO:20:Train Epoch: 1, 920/2646 (35%), Loss: 6.628529\n",
            "2022-12-14 02:56:26,189 [INFO]: Train Epoch: 1, 940/2646 (36%), Loss: 6.533363\n",
            "INFO:20:Train Epoch: 1, 940/2646 (36%), Loss: 6.533363\n",
            "2022-12-14 02:56:28,991 [INFO]: Train Epoch: 1, 960/2646 (36%), Loss: 6.650873\n",
            "INFO:20:Train Epoch: 1, 960/2646 (36%), Loss: 6.650873\n",
            "2022-12-14 02:56:31,943 [INFO]: Train Epoch: 1, 980/2646 (37%), Loss: 6.603246\n",
            "INFO:20:Train Epoch: 1, 980/2646 (37%), Loss: 6.603246\n",
            "2022-12-14 02:56:34,736 [INFO]: Train Epoch: 1, 1000/2646 (38%), Loss: 6.811238\n",
            "INFO:20:Train Epoch: 1, 1000/2646 (38%), Loss: 6.811238\n",
            "2022-12-14 02:56:37,575 [INFO]: Train Epoch: 1, 1020/2646 (39%), Loss: 6.656311\n",
            "INFO:20:Train Epoch: 1, 1020/2646 (39%), Loss: 6.656311\n",
            "2022-12-14 02:56:40,340 [INFO]: Train Epoch: 1, 1040/2646 (39%), Loss: 6.820594\n",
            "INFO:20:Train Epoch: 1, 1040/2646 (39%), Loss: 6.820594\n",
            "2022-12-14 02:56:43,183 [INFO]: Train Epoch: 1, 1060/2646 (40%), Loss: 6.715974\n",
            "INFO:20:Train Epoch: 1, 1060/2646 (40%), Loss: 6.715974\n",
            "2022-12-14 02:56:45,890 [INFO]: Train Epoch: 1, 1080/2646 (41%), Loss: 6.705903\n",
            "INFO:20:Train Epoch: 1, 1080/2646 (41%), Loss: 6.705903\n",
            "2022-12-14 02:56:48,644 [INFO]: Train Epoch: 1, 1100/2646 (42%), Loss: 6.820162\n",
            "INFO:20:Train Epoch: 1, 1100/2646 (42%), Loss: 6.820162\n",
            "2022-12-14 02:56:51,439 [INFO]: Train Epoch: 1, 1120/2646 (42%), Loss: 6.787555\n",
            "INFO:20:Train Epoch: 1, 1120/2646 (42%), Loss: 6.787555\n",
            "2022-12-14 02:56:54,187 [INFO]: Train Epoch: 1, 1140/2646 (43%), Loss: 6.733576\n",
            "INFO:20:Train Epoch: 1, 1140/2646 (43%), Loss: 6.733576\n",
            "2022-12-14 02:56:57,080 [INFO]: Train Epoch: 1, 1160/2646 (44%), Loss: 6.703362\n",
            "INFO:20:Train Epoch: 1, 1160/2646 (44%), Loss: 6.703362\n",
            "2022-12-14 02:56:59,903 [INFO]: Train Epoch: 1, 1180/2646 (45%), Loss: 6.663344\n",
            "INFO:20:Train Epoch: 1, 1180/2646 (45%), Loss: 6.663344\n",
            "2022-12-14 02:57:02,781 [INFO]: Train Epoch: 1, 1200/2646 (45%), Loss: 6.725129\n",
            "INFO:20:Train Epoch: 1, 1200/2646 (45%), Loss: 6.725129\n",
            "2022-12-14 02:57:05,453 [INFO]: Train Epoch: 1, 1220/2646 (46%), Loss: 6.545072\n",
            "INFO:20:Train Epoch: 1, 1220/2646 (46%), Loss: 6.545072\n",
            "2022-12-14 02:57:08,261 [INFO]: Train Epoch: 1, 1240/2646 (47%), Loss: 6.790607\n",
            "INFO:20:Train Epoch: 1, 1240/2646 (47%), Loss: 6.790607\n",
            "2022-12-14 02:57:10,946 [INFO]: Train Epoch: 1, 1260/2646 (48%), Loss: 6.752557\n",
            "INFO:20:Train Epoch: 1, 1260/2646 (48%), Loss: 6.752557\n",
            "2022-12-14 02:57:13,887 [INFO]: Train Epoch: 1, 1280/2646 (48%), Loss: 6.660856\n",
            "INFO:20:Train Epoch: 1, 1280/2646 (48%), Loss: 6.660856\n",
            "2022-12-14 02:57:16,691 [INFO]: Train Epoch: 1, 1300/2646 (49%), Loss: 6.687278\n",
            "INFO:20:Train Epoch: 1, 1300/2646 (49%), Loss: 6.687278\n",
            "2022-12-14 02:57:19,520 [INFO]: Train Epoch: 1, 1320/2646 (50%), Loss: 6.624936\n",
            "INFO:20:Train Epoch: 1, 1320/2646 (50%), Loss: 6.624936\n",
            "2022-12-14 02:57:22,358 [INFO]: Train Epoch: 1, 1340/2646 (51%), Loss: 6.670627\n",
            "INFO:20:Train Epoch: 1, 1340/2646 (51%), Loss: 6.670627\n",
            "2022-12-14 02:57:25,235 [INFO]: Train Epoch: 1, 1360/2646 (51%), Loss: 6.657543\n",
            "INFO:20:Train Epoch: 1, 1360/2646 (51%), Loss: 6.657543\n",
            "2022-12-14 02:57:27,941 [INFO]: Train Epoch: 1, 1380/2646 (52%), Loss: 6.600138\n",
            "INFO:20:Train Epoch: 1, 1380/2646 (52%), Loss: 6.600138\n",
            "2022-12-14 02:57:30,932 [INFO]: Train Epoch: 1, 1400/2646 (53%), Loss: 6.619115\n",
            "INFO:20:Train Epoch: 1, 1400/2646 (53%), Loss: 6.619115\n",
            "2022-12-14 02:57:33,749 [INFO]: Train Epoch: 1, 1420/2646 (54%), Loss: 6.665089\n",
            "INFO:20:Train Epoch: 1, 1420/2646 (54%), Loss: 6.665089\n",
            "2022-12-14 02:57:36,678 [INFO]: Train Epoch: 1, 1440/2646 (54%), Loss: 6.665967\n",
            "INFO:20:Train Epoch: 1, 1440/2646 (54%), Loss: 6.665967\n",
            "2022-12-14 02:57:39,490 [INFO]: Train Epoch: 1, 1460/2646 (55%), Loss: 6.818274\n",
            "INFO:20:Train Epoch: 1, 1460/2646 (55%), Loss: 6.818274\n",
            "2022-12-14 02:57:42,291 [INFO]: Train Epoch: 1, 1480/2646 (56%), Loss: 6.698493\n",
            "INFO:20:Train Epoch: 1, 1480/2646 (56%), Loss: 6.698493\n",
            "2022-12-14 02:57:45,121 [INFO]: Train Epoch: 1, 1500/2646 (57%), Loss: 6.656515\n",
            "INFO:20:Train Epoch: 1, 1500/2646 (57%), Loss: 6.656515\n",
            "2022-12-14 02:57:48,041 [INFO]: Train Epoch: 1, 1520/2646 (57%), Loss: 6.805430\n",
            "INFO:20:Train Epoch: 1, 1520/2646 (57%), Loss: 6.805430\n",
            "2022-12-14 02:57:50,957 [INFO]: Train Epoch: 1, 1540/2646 (58%), Loss: 6.519192\n",
            "INFO:20:Train Epoch: 1, 1540/2646 (58%), Loss: 6.519192\n",
            "2022-12-14 02:57:53,781 [INFO]: Train Epoch: 1, 1560/2646 (59%), Loss: 6.690748\n",
            "INFO:20:Train Epoch: 1, 1560/2646 (59%), Loss: 6.690748\n",
            "2022-12-14 02:57:56,734 [INFO]: Train Epoch: 1, 1580/2646 (60%), Loss: 6.683772\n",
            "INFO:20:Train Epoch: 1, 1580/2646 (60%), Loss: 6.683772\n",
            "2022-12-14 02:57:59,537 [INFO]: Train Epoch: 1, 1600/2646 (60%), Loss: 6.585929\n",
            "INFO:20:Train Epoch: 1, 1600/2646 (60%), Loss: 6.585929\n",
            "2022-12-14 02:58:02,336 [INFO]: Train Epoch: 1, 1620/2646 (61%), Loss: 6.683993\n",
            "INFO:20:Train Epoch: 1, 1620/2646 (61%), Loss: 6.683993\n",
            "2022-12-14 02:58:05,136 [INFO]: Train Epoch: 1, 1640/2646 (62%), Loss: 6.673688\n",
            "INFO:20:Train Epoch: 1, 1640/2646 (62%), Loss: 6.673688\n",
            "2022-12-14 02:58:08,043 [INFO]: Train Epoch: 1, 1660/2646 (63%), Loss: 6.626728\n",
            "INFO:20:Train Epoch: 1, 1660/2646 (63%), Loss: 6.626728\n",
            "2022-12-14 02:58:10,905 [INFO]: Train Epoch: 1, 1680/2646 (63%), Loss: 6.710300\n",
            "INFO:20:Train Epoch: 1, 1680/2646 (63%), Loss: 6.710300\n",
            "2022-12-14 02:58:13,635 [INFO]: Train Epoch: 1, 1700/2646 (64%), Loss: 6.603263\n",
            "INFO:20:Train Epoch: 1, 1700/2646 (64%), Loss: 6.603263\n",
            "2022-12-14 02:58:16,455 [INFO]: Train Epoch: 1, 1720/2646 (65%), Loss: 6.802140\n",
            "INFO:20:Train Epoch: 1, 1720/2646 (65%), Loss: 6.802140\n",
            "2022-12-14 02:58:19,434 [INFO]: Train Epoch: 1, 1740/2646 (66%), Loss: 6.619353\n",
            "INFO:20:Train Epoch: 1, 1740/2646 (66%), Loss: 6.619353\n",
            "2022-12-14 02:58:22,287 [INFO]: Train Epoch: 1, 1760/2646 (67%), Loss: 6.625750\n",
            "INFO:20:Train Epoch: 1, 1760/2646 (67%), Loss: 6.625750\n",
            "2022-12-14 02:58:25,239 [INFO]: Train Epoch: 1, 1780/2646 (67%), Loss: 6.541704\n",
            "INFO:20:Train Epoch: 1, 1780/2646 (67%), Loss: 6.541704\n",
            "2022-12-14 02:58:28,136 [INFO]: Train Epoch: 1, 1800/2646 (68%), Loss: 6.651278\n",
            "INFO:20:Train Epoch: 1, 1800/2646 (68%), Loss: 6.651278\n",
            "2022-12-14 02:58:30,909 [INFO]: Train Epoch: 1, 1820/2646 (69%), Loss: 6.662149\n",
            "INFO:20:Train Epoch: 1, 1820/2646 (69%), Loss: 6.662149\n",
            "2022-12-14 02:58:33,827 [INFO]: Train Epoch: 1, 1840/2646 (70%), Loss: 6.555017\n",
            "INFO:20:Train Epoch: 1, 1840/2646 (70%), Loss: 6.555017\n",
            "2022-12-14 02:58:36,618 [INFO]: Train Epoch: 1, 1860/2646 (70%), Loss: 6.574227\n",
            "INFO:20:Train Epoch: 1, 1860/2646 (70%), Loss: 6.574227\n",
            "2022-12-14 02:58:39,293 [INFO]: Train Epoch: 1, 1880/2646 (71%), Loss: 6.657708\n",
            "INFO:20:Train Epoch: 1, 1880/2646 (71%), Loss: 6.657708\n",
            "2022-12-14 02:58:41,965 [INFO]: Train Epoch: 1, 1900/2646 (72%), Loss: 6.598457\n",
            "INFO:20:Train Epoch: 1, 1900/2646 (72%), Loss: 6.598457\n",
            "2022-12-14 02:58:44,900 [INFO]: Train Epoch: 1, 1920/2646 (73%), Loss: 6.602044\n",
            "INFO:20:Train Epoch: 1, 1920/2646 (73%), Loss: 6.602044\n",
            "2022-12-14 02:58:47,647 [INFO]: Train Epoch: 1, 1940/2646 (73%), Loss: 6.653117\n",
            "INFO:20:Train Epoch: 1, 1940/2646 (73%), Loss: 6.653117\n",
            "2022-12-14 02:58:50,408 [INFO]: Train Epoch: 1, 1960/2646 (74%), Loss: 6.549245\n",
            "INFO:20:Train Epoch: 1, 1960/2646 (74%), Loss: 6.549245\n",
            "2022-12-14 02:58:53,420 [INFO]: Train Epoch: 1, 1980/2646 (75%), Loss: 6.639801\n",
            "INFO:20:Train Epoch: 1, 1980/2646 (75%), Loss: 6.639801\n",
            "2022-12-14 02:58:56,204 [INFO]: Train Epoch: 1, 2000/2646 (76%), Loss: 6.663714\n",
            "INFO:20:Train Epoch: 1, 2000/2646 (76%), Loss: 6.663714\n",
            "2022-12-14 02:58:59,046 [INFO]: Train Epoch: 1, 2020/2646 (76%), Loss: 6.739740\n",
            "INFO:20:Train Epoch: 1, 2020/2646 (76%), Loss: 6.739740\n",
            "2022-12-14 02:59:01,830 [INFO]: Train Epoch: 1, 2040/2646 (77%), Loss: 6.574836\n",
            "INFO:20:Train Epoch: 1, 2040/2646 (77%), Loss: 6.574836\n",
            "2022-12-14 02:59:04,766 [INFO]: Train Epoch: 1, 2060/2646 (78%), Loss: 6.568011\n",
            "INFO:20:Train Epoch: 1, 2060/2646 (78%), Loss: 6.568011\n",
            "2022-12-14 02:59:07,538 [INFO]: Train Epoch: 1, 2080/2646 (79%), Loss: 6.769816\n",
            "INFO:20:Train Epoch: 1, 2080/2646 (79%), Loss: 6.769816\n",
            "2022-12-14 02:59:10,322 [INFO]: Train Epoch: 1, 2100/2646 (79%), Loss: 6.709146\n",
            "INFO:20:Train Epoch: 1, 2100/2646 (79%), Loss: 6.709146\n",
            "2022-12-14 02:59:13,193 [INFO]: Train Epoch: 1, 2120/2646 (80%), Loss: 6.609571\n",
            "INFO:20:Train Epoch: 1, 2120/2646 (80%), Loss: 6.609571\n",
            "2022-12-14 02:59:16,210 [INFO]: Train Epoch: 1, 2140/2646 (81%), Loss: 6.636405\n",
            "INFO:20:Train Epoch: 1, 2140/2646 (81%), Loss: 6.636405\n",
            "2022-12-14 02:59:18,965 [INFO]: Train Epoch: 1, 2160/2646 (82%), Loss: 6.462743\n",
            "INFO:20:Train Epoch: 1, 2160/2646 (82%), Loss: 6.462743\n",
            "2022-12-14 02:59:21,727 [INFO]: Train Epoch: 1, 2180/2646 (82%), Loss: 6.695687\n",
            "INFO:20:Train Epoch: 1, 2180/2646 (82%), Loss: 6.695687\n",
            "2022-12-14 02:59:24,562 [INFO]: Train Epoch: 1, 2200/2646 (83%), Loss: 6.661783\n",
            "INFO:20:Train Epoch: 1, 2200/2646 (83%), Loss: 6.661783\n",
            "2022-12-14 02:59:27,308 [INFO]: Train Epoch: 1, 2220/2646 (84%), Loss: 6.671407\n",
            "INFO:20:Train Epoch: 1, 2220/2646 (84%), Loss: 6.671407\n",
            "2022-12-14 02:59:30,101 [INFO]: Train Epoch: 1, 2240/2646 (85%), Loss: 6.804186\n",
            "INFO:20:Train Epoch: 1, 2240/2646 (85%), Loss: 6.804186\n",
            "2022-12-14 02:59:32,908 [INFO]: Train Epoch: 1, 2260/2646 (85%), Loss: 6.620553\n",
            "INFO:20:Train Epoch: 1, 2260/2646 (85%), Loss: 6.620553\n",
            "2022-12-14 02:59:35,652 [INFO]: Train Epoch: 1, 2280/2646 (86%), Loss: 6.616456\n",
            "INFO:20:Train Epoch: 1, 2280/2646 (86%), Loss: 6.616456\n",
            "2022-12-14 02:59:38,591 [INFO]: Train Epoch: 1, 2300/2646 (87%), Loss: 6.618901\n",
            "INFO:20:Train Epoch: 1, 2300/2646 (87%), Loss: 6.618901\n",
            "2022-12-14 02:59:41,442 [INFO]: Train Epoch: 1, 2320/2646 (88%), Loss: 6.621271\n",
            "INFO:20:Train Epoch: 1, 2320/2646 (88%), Loss: 6.621271\n",
            "2022-12-14 02:59:44,102 [INFO]: Train Epoch: 1, 2340/2646 (88%), Loss: 6.534698\n",
            "INFO:20:Train Epoch: 1, 2340/2646 (88%), Loss: 6.534698\n",
            "2022-12-14 02:59:47,049 [INFO]: Train Epoch: 1, 2360/2646 (89%), Loss: 6.580227\n",
            "INFO:20:Train Epoch: 1, 2360/2646 (89%), Loss: 6.580227\n",
            "2022-12-14 02:59:49,892 [INFO]: Train Epoch: 1, 2380/2646 (90%), Loss: 6.552228\n",
            "INFO:20:Train Epoch: 1, 2380/2646 (90%), Loss: 6.552228\n",
            "2022-12-14 02:59:52,729 [INFO]: Train Epoch: 1, 2400/2646 (91%), Loss: 6.680364\n",
            "INFO:20:Train Epoch: 1, 2400/2646 (91%), Loss: 6.680364\n",
            "2022-12-14 02:59:55,792 [INFO]: Train Epoch: 1, 2420/2646 (91%), Loss: 6.628530\n",
            "INFO:20:Train Epoch: 1, 2420/2646 (91%), Loss: 6.628530\n",
            "2022-12-14 02:59:58,777 [INFO]: Train Epoch: 1, 2440/2646 (92%), Loss: 6.646349\n",
            "INFO:20:Train Epoch: 1, 2440/2646 (92%), Loss: 6.646349\n",
            "2022-12-14 03:00:01,602 [INFO]: Train Epoch: 1, 2460/2646 (93%), Loss: 6.712057\n",
            "INFO:20:Train Epoch: 1, 2460/2646 (93%), Loss: 6.712057\n",
            "2022-12-14 03:00:04,379 [INFO]: Train Epoch: 1, 2480/2646 (94%), Loss: 6.693581\n",
            "INFO:20:Train Epoch: 1, 2480/2646 (94%), Loss: 6.693581\n",
            "2022-12-14 03:00:07,247 [INFO]: Train Epoch: 1, 2500/2646 (94%), Loss: 6.662197\n",
            "INFO:20:Train Epoch: 1, 2500/2646 (94%), Loss: 6.662197\n",
            "2022-12-14 03:00:10,041 [INFO]: Train Epoch: 1, 2520/2646 (95%), Loss: 6.675192\n",
            "INFO:20:Train Epoch: 1, 2520/2646 (95%), Loss: 6.675192\n",
            "2022-12-14 03:00:12,985 [INFO]: Train Epoch: 1, 2540/2646 (96%), Loss: 6.645176\n",
            "INFO:20:Train Epoch: 1, 2540/2646 (96%), Loss: 6.645176\n",
            "2022-12-14 03:00:15,913 [INFO]: Train Epoch: 1, 2560/2646 (97%), Loss: 6.572751\n",
            "INFO:20:Train Epoch: 1, 2560/2646 (97%), Loss: 6.572751\n",
            "2022-12-14 03:00:18,739 [INFO]: Train Epoch: 1, 2580/2646 (98%), Loss: 6.618763\n",
            "INFO:20:Train Epoch: 1, 2580/2646 (98%), Loss: 6.618763\n",
            "2022-12-14 03:00:21,589 [INFO]: Train Epoch: 1, 2600/2646 (98%), Loss: 6.816565\n",
            "INFO:20:Train Epoch: 1, 2600/2646 (98%), Loss: 6.816565\n",
            "2022-12-14 03:00:24,386 [INFO]: Train Epoch: 1, 2620/2646 (99%), Loss: 6.715784\n",
            "INFO:20:Train Epoch: 1, 2620/2646 (99%), Loss: 6.715784\n",
            "2022-12-14 03:00:27,234 [INFO]: Train Epoch: 1, 2640/2646 (100%), Loss: 6.597384\n",
            "INFO:20:Train Epoch: 1, 2640/2646 (100%), Loss: 6.597384\n",
            "2022-12-14 03:00:27,781 [INFO]: Train Epoch: 1, total Loss: 17652.888187, mean Loss: 6.671537\n",
            "INFO:20:Train Epoch: 1, total Loss: 17652.888187, mean Loss: 6.671537\n",
            "2022-12-14 03:00:27,781 [DEBUG]: start validation\n",
            "DEBUG:10:start validation\n",
            "2022-12-14 03:00:30,382 [INFO]: Train Epoch: 1, validation loss is : 7.296\n",
            "INFO:20:Train Epoch: 1, validation loss is : 7.296\n",
            "Directory ./saved_models do not exist; creating...\n",
            "2022-12-14 03:00:31,515 [DEBUG]: Saving current best: ./saved_models/model_best.pt...\n",
            "DEBUG:10:Saving current best: ./saved_models/model_best.pt...\n",
            "2022-12-14 03:00:31,515 [DEBUG]: #######: 2 ...\n",
            "DEBUG:10:#######: 2 ...\n",
            "2022-12-14 03:00:31,647 [INFO]: Train Epoch: 2, 0/2646 (0%), Loss: 6.642112\n",
            "INFO:20:Train Epoch: 2, 0/2646 (0%), Loss: 6.642112\n",
            "2022-12-14 03:00:34,339 [INFO]: Train Epoch: 2, 20/2646 (1%), Loss: 6.521063\n",
            "INFO:20:Train Epoch: 2, 20/2646 (1%), Loss: 6.521063\n",
            "2022-12-14 03:00:37,136 [INFO]: Train Epoch: 2, 40/2646 (2%), Loss: 6.635936\n",
            "INFO:20:Train Epoch: 2, 40/2646 (2%), Loss: 6.635936\n",
            "2022-12-14 03:00:39,869 [INFO]: Train Epoch: 2, 60/2646 (2%), Loss: 6.510801\n",
            "INFO:20:Train Epoch: 2, 60/2646 (2%), Loss: 6.510801\n",
            "2022-12-14 03:00:42,692 [INFO]: Train Epoch: 2, 80/2646 (3%), Loss: 6.731062\n",
            "INFO:20:Train Epoch: 2, 80/2646 (3%), Loss: 6.731062\n",
            "2022-12-14 03:00:45,435 [INFO]: Train Epoch: 2, 100/2646 (4%), Loss: 6.687920\n",
            "INFO:20:Train Epoch: 2, 100/2646 (4%), Loss: 6.687920\n",
            "2022-12-14 03:00:48,229 [INFO]: Train Epoch: 2, 120/2646 (5%), Loss: 6.626839\n",
            "INFO:20:Train Epoch: 2, 120/2646 (5%), Loss: 6.626839\n",
            "2022-12-14 03:00:50,936 [INFO]: Train Epoch: 2, 140/2646 (5%), Loss: 6.783462\n",
            "INFO:20:Train Epoch: 2, 140/2646 (5%), Loss: 6.783462\n",
            "2022-12-14 03:00:53,734 [INFO]: Train Epoch: 2, 160/2646 (6%), Loss: 6.647614\n",
            "INFO:20:Train Epoch: 2, 160/2646 (6%), Loss: 6.647614\n",
            "2022-12-14 03:00:56,399 [INFO]: Train Epoch: 2, 180/2646 (7%), Loss: 6.555606\n",
            "INFO:20:Train Epoch: 2, 180/2646 (7%), Loss: 6.555606\n",
            "2022-12-14 03:00:59,215 [INFO]: Train Epoch: 2, 200/2646 (8%), Loss: 6.647515\n",
            "INFO:20:Train Epoch: 2, 200/2646 (8%), Loss: 6.647515\n",
            "2022-12-14 03:01:02,184 [INFO]: Train Epoch: 2, 220/2646 (8%), Loss: 6.633679\n",
            "INFO:20:Train Epoch: 2, 220/2646 (8%), Loss: 6.633679\n",
            "2022-12-14 03:01:05,020 [INFO]: Train Epoch: 2, 240/2646 (9%), Loss: 6.652454\n",
            "INFO:20:Train Epoch: 2, 240/2646 (9%), Loss: 6.652454\n",
            "2022-12-14 03:01:07,752 [INFO]: Train Epoch: 2, 260/2646 (10%), Loss: 6.641329\n",
            "INFO:20:Train Epoch: 2, 260/2646 (10%), Loss: 6.641329\n",
            "2022-12-14 03:01:10,486 [INFO]: Train Epoch: 2, 280/2646 (11%), Loss: 6.615566\n",
            "INFO:20:Train Epoch: 2, 280/2646 (11%), Loss: 6.615566\n",
            "2022-12-14 03:01:13,246 [INFO]: Train Epoch: 2, 300/2646 (11%), Loss: 6.613959\n",
            "INFO:20:Train Epoch: 2, 300/2646 (11%), Loss: 6.613959\n",
            "2022-12-14 03:01:15,967 [INFO]: Train Epoch: 2, 320/2646 (12%), Loss: 6.755024\n",
            "INFO:20:Train Epoch: 2, 320/2646 (12%), Loss: 6.755024\n",
            "2022-12-14 03:01:18,766 [INFO]: Train Epoch: 2, 340/2646 (13%), Loss: 6.706261\n",
            "INFO:20:Train Epoch: 2, 340/2646 (13%), Loss: 6.706261\n",
            "2022-12-14 03:01:21,802 [INFO]: Train Epoch: 2, 360/2646 (14%), Loss: 6.552312\n",
            "INFO:20:Train Epoch: 2, 360/2646 (14%), Loss: 6.552312\n",
            "2022-12-14 03:01:24,629 [INFO]: Train Epoch: 2, 380/2646 (14%), Loss: 6.601497\n",
            "INFO:20:Train Epoch: 2, 380/2646 (14%), Loss: 6.601497\n",
            "2022-12-14 03:01:27,519 [INFO]: Train Epoch: 2, 400/2646 (15%), Loss: 6.684982\n",
            "INFO:20:Train Epoch: 2, 400/2646 (15%), Loss: 6.684982\n",
            "2022-12-14 03:01:30,434 [INFO]: Train Epoch: 2, 420/2646 (16%), Loss: 6.469107\n",
            "INFO:20:Train Epoch: 2, 420/2646 (16%), Loss: 6.469107\n",
            "2022-12-14 03:01:33,290 [INFO]: Train Epoch: 2, 440/2646 (17%), Loss: 6.616532\n",
            "INFO:20:Train Epoch: 2, 440/2646 (17%), Loss: 6.616532\n",
            "2022-12-14 03:01:36,036 [INFO]: Train Epoch: 2, 460/2646 (17%), Loss: 6.679790\n",
            "INFO:20:Train Epoch: 2, 460/2646 (17%), Loss: 6.679790\n",
            "2022-12-14 03:01:39,090 [INFO]: Train Epoch: 2, 480/2646 (18%), Loss: 6.628386\n",
            "INFO:20:Train Epoch: 2, 480/2646 (18%), Loss: 6.628386\n",
            "2022-12-14 03:01:41,980 [INFO]: Train Epoch: 2, 500/2646 (19%), Loss: 6.587986\n",
            "INFO:20:Train Epoch: 2, 500/2646 (19%), Loss: 6.587986\n",
            "2022-12-14 03:01:45,061 [INFO]: Train Epoch: 2, 520/2646 (20%), Loss: 6.539561\n",
            "INFO:20:Train Epoch: 2, 520/2646 (20%), Loss: 6.539561\n",
            "2022-12-14 03:01:48,028 [INFO]: Train Epoch: 2, 540/2646 (20%), Loss: 6.618204\n",
            "INFO:20:Train Epoch: 2, 540/2646 (20%), Loss: 6.618204\n",
            "2022-12-14 03:01:51,015 [INFO]: Train Epoch: 2, 560/2646 (21%), Loss: 6.681048\n",
            "INFO:20:Train Epoch: 2, 560/2646 (21%), Loss: 6.681048\n",
            "2022-12-14 03:01:53,990 [INFO]: Train Epoch: 2, 580/2646 (22%), Loss: 6.599278\n",
            "INFO:20:Train Epoch: 2, 580/2646 (22%), Loss: 6.599278\n",
            "2022-12-14 03:01:56,821 [INFO]: Train Epoch: 2, 600/2646 (23%), Loss: 6.549555\n",
            "INFO:20:Train Epoch: 2, 600/2646 (23%), Loss: 6.549555\n",
            "2022-12-14 03:01:59,678 [INFO]: Train Epoch: 2, 620/2646 (23%), Loss: 6.543259\n",
            "INFO:20:Train Epoch: 2, 620/2646 (23%), Loss: 6.543259\n",
            "2022-12-14 03:02:02,446 [INFO]: Train Epoch: 2, 640/2646 (24%), Loss: 6.523248\n",
            "INFO:20:Train Epoch: 2, 640/2646 (24%), Loss: 6.523248\n",
            "2022-12-14 03:02:05,275 [INFO]: Train Epoch: 2, 660/2646 (25%), Loss: 6.585566\n",
            "INFO:20:Train Epoch: 2, 660/2646 (25%), Loss: 6.585566\n",
            "2022-12-14 03:02:08,048 [INFO]: Train Epoch: 2, 680/2646 (26%), Loss: 6.720865\n",
            "INFO:20:Train Epoch: 2, 680/2646 (26%), Loss: 6.720865\n",
            "2022-12-14 03:02:10,817 [INFO]: Train Epoch: 2, 700/2646 (26%), Loss: 6.589105\n",
            "INFO:20:Train Epoch: 2, 700/2646 (26%), Loss: 6.589105\n",
            "2022-12-14 03:02:13,512 [INFO]: Train Epoch: 2, 720/2646 (27%), Loss: 6.638452\n",
            "INFO:20:Train Epoch: 2, 720/2646 (27%), Loss: 6.638452\n",
            "2022-12-14 03:02:16,332 [INFO]: Train Epoch: 2, 740/2646 (28%), Loss: 6.546731\n",
            "INFO:20:Train Epoch: 2, 740/2646 (28%), Loss: 6.546731\n",
            "2022-12-14 03:02:19,159 [INFO]: Train Epoch: 2, 760/2646 (29%), Loss: 6.677396\n",
            "INFO:20:Train Epoch: 2, 760/2646 (29%), Loss: 6.677396\n",
            "2022-12-14 03:02:21,987 [INFO]: Train Epoch: 2, 780/2646 (29%), Loss: 6.677676\n",
            "INFO:20:Train Epoch: 2, 780/2646 (29%), Loss: 6.677676\n",
            "2022-12-14 03:02:24,940 [INFO]: Train Epoch: 2, 800/2646 (30%), Loss: 6.662660\n",
            "INFO:20:Train Epoch: 2, 800/2646 (30%), Loss: 6.662660\n",
            "2022-12-14 03:02:27,777 [INFO]: Train Epoch: 2, 820/2646 (31%), Loss: 6.635850\n",
            "INFO:20:Train Epoch: 2, 820/2646 (31%), Loss: 6.635850\n",
            "2022-12-14 03:02:30,485 [INFO]: Train Epoch: 2, 840/2646 (32%), Loss: 6.604902\n",
            "INFO:20:Train Epoch: 2, 840/2646 (32%), Loss: 6.604902\n",
            "2022-12-14 03:02:33,252 [INFO]: Train Epoch: 2, 860/2646 (33%), Loss: 6.540731\n",
            "INFO:20:Train Epoch: 2, 860/2646 (33%), Loss: 6.540731\n",
            "2022-12-14 03:02:36,065 [INFO]: Train Epoch: 2, 880/2646 (33%), Loss: 6.666012\n",
            "INFO:20:Train Epoch: 2, 880/2646 (33%), Loss: 6.666012\n",
            "2022-12-14 03:02:38,901 [INFO]: Train Epoch: 2, 900/2646 (34%), Loss: 6.590177\n",
            "INFO:20:Train Epoch: 2, 900/2646 (34%), Loss: 6.590177\n",
            "2022-12-14 03:02:41,693 [INFO]: Train Epoch: 2, 920/2646 (35%), Loss: 6.660257\n",
            "INFO:20:Train Epoch: 2, 920/2646 (35%), Loss: 6.660257\n",
            "2022-12-14 03:02:44,521 [INFO]: Train Epoch: 2, 940/2646 (36%), Loss: 6.531098\n",
            "INFO:20:Train Epoch: 2, 940/2646 (36%), Loss: 6.531098\n",
            "2022-12-14 03:02:47,315 [INFO]: Train Epoch: 2, 960/2646 (36%), Loss: 6.651152\n",
            "INFO:20:Train Epoch: 2, 960/2646 (36%), Loss: 6.651152\n",
            "2022-12-14 03:02:50,352 [INFO]: Train Epoch: 2, 980/2646 (37%), Loss: 6.649021\n",
            "INFO:20:Train Epoch: 2, 980/2646 (37%), Loss: 6.649021\n",
            "2022-12-14 03:02:53,206 [INFO]: Train Epoch: 2, 1000/2646 (38%), Loss: 6.676991\n",
            "INFO:20:Train Epoch: 2, 1000/2646 (38%), Loss: 6.676991\n",
            "2022-12-14 03:02:56,047 [INFO]: Train Epoch: 2, 1020/2646 (39%), Loss: 6.546286\n",
            "INFO:20:Train Epoch: 2, 1020/2646 (39%), Loss: 6.546286\n",
            "2022-12-14 03:02:58,820 [INFO]: Train Epoch: 2, 1040/2646 (39%), Loss: 6.679102\n",
            "INFO:20:Train Epoch: 2, 1040/2646 (39%), Loss: 6.679102\n",
            "2022-12-14 03:03:01,565 [INFO]: Train Epoch: 2, 1060/2646 (40%), Loss: 6.602722\n",
            "INFO:20:Train Epoch: 2, 1060/2646 (40%), Loss: 6.602722\n",
            "2022-12-14 03:03:04,198 [INFO]: Train Epoch: 2, 1080/2646 (41%), Loss: 6.608299\n",
            "INFO:20:Train Epoch: 2, 1080/2646 (41%), Loss: 6.608299\n",
            "2022-12-14 03:03:06,951 [INFO]: Train Epoch: 2, 1100/2646 (42%), Loss: 6.730036\n",
            "INFO:20:Train Epoch: 2, 1100/2646 (42%), Loss: 6.730036\n",
            "2022-12-14 03:03:09,750 [INFO]: Train Epoch: 2, 1120/2646 (42%), Loss: 6.688898\n",
            "INFO:20:Train Epoch: 2, 1120/2646 (42%), Loss: 6.688898\n",
            "2022-12-14 03:03:12,488 [INFO]: Train Epoch: 2, 1140/2646 (43%), Loss: 6.674856\n",
            "INFO:20:Train Epoch: 2, 1140/2646 (43%), Loss: 6.674856\n",
            "2022-12-14 03:03:15,382 [INFO]: Train Epoch: 2, 1160/2646 (44%), Loss: 6.628518\n",
            "INFO:20:Train Epoch: 2, 1160/2646 (44%), Loss: 6.628518\n",
            "2022-12-14 03:03:18,198 [INFO]: Train Epoch: 2, 1180/2646 (45%), Loss: 6.559918\n",
            "INFO:20:Train Epoch: 2, 1180/2646 (45%), Loss: 6.559918\n",
            "2022-12-14 03:03:21,078 [INFO]: Train Epoch: 2, 1200/2646 (45%), Loss: 6.615658\n",
            "INFO:20:Train Epoch: 2, 1200/2646 (45%), Loss: 6.615658\n",
            "2022-12-14 03:03:23,758 [INFO]: Train Epoch: 2, 1220/2646 (46%), Loss: 6.454840\n",
            "INFO:20:Train Epoch: 2, 1220/2646 (46%), Loss: 6.454840\n",
            "2022-12-14 03:03:26,572 [INFO]: Train Epoch: 2, 1240/2646 (47%), Loss: 6.678465\n",
            "INFO:20:Train Epoch: 2, 1240/2646 (47%), Loss: 6.678465\n",
            "2022-12-14 03:03:29,283 [INFO]: Train Epoch: 2, 1260/2646 (48%), Loss: 6.674043\n",
            "INFO:20:Train Epoch: 2, 1260/2646 (48%), Loss: 6.674043\n",
            "2022-12-14 03:03:32,228 [INFO]: Train Epoch: 2, 1280/2646 (48%), Loss: 6.588552\n",
            "INFO:20:Train Epoch: 2, 1280/2646 (48%), Loss: 6.588552\n",
            "2022-12-14 03:03:35,022 [INFO]: Train Epoch: 2, 1300/2646 (49%), Loss: 6.624673\n",
            "INFO:20:Train Epoch: 2, 1300/2646 (49%), Loss: 6.624673\n",
            "2022-12-14 03:03:37,857 [INFO]: Train Epoch: 2, 1320/2646 (50%), Loss: 6.603355\n",
            "INFO:20:Train Epoch: 2, 1320/2646 (50%), Loss: 6.603355\n",
            "2022-12-14 03:03:40,705 [INFO]: Train Epoch: 2, 1340/2646 (51%), Loss: 6.581710\n",
            "INFO:20:Train Epoch: 2, 1340/2646 (51%), Loss: 6.581710\n",
            "2022-12-14 03:03:43,586 [INFO]: Train Epoch: 2, 1360/2646 (51%), Loss: 6.601787\n",
            "INFO:20:Train Epoch: 2, 1360/2646 (51%), Loss: 6.601787\n",
            "2022-12-14 03:03:46,301 [INFO]: Train Epoch: 2, 1380/2646 (52%), Loss: 6.509587\n",
            "INFO:20:Train Epoch: 2, 1380/2646 (52%), Loss: 6.509587\n",
            "2022-12-14 03:03:49,289 [INFO]: Train Epoch: 2, 1400/2646 (53%), Loss: 6.570745\n",
            "INFO:20:Train Epoch: 2, 1400/2646 (53%), Loss: 6.570745\n",
            "2022-12-14 03:03:52,109 [INFO]: Train Epoch: 2, 1420/2646 (54%), Loss: 6.607858\n",
            "INFO:20:Train Epoch: 2, 1420/2646 (54%), Loss: 6.607858\n",
            "2022-12-14 03:03:55,027 [INFO]: Train Epoch: 2, 1440/2646 (54%), Loss: 6.607880\n",
            "INFO:20:Train Epoch: 2, 1440/2646 (54%), Loss: 6.607880\n",
            "2022-12-14 03:03:57,829 [INFO]: Train Epoch: 2, 1460/2646 (55%), Loss: 6.765388\n",
            "INFO:20:Train Epoch: 2, 1460/2646 (55%), Loss: 6.765388\n",
            "2022-12-14 03:04:00,639 [INFO]: Train Epoch: 2, 1480/2646 (56%), Loss: 6.627857\n",
            "INFO:20:Train Epoch: 2, 1480/2646 (56%), Loss: 6.627857\n",
            "2022-12-14 03:04:03,784 [INFO]: Train Epoch: 2, 1500/2646 (57%), Loss: 6.583202\n",
            "INFO:20:Train Epoch: 2, 1500/2646 (57%), Loss: 6.583202\n",
            "2022-12-14 03:04:06,857 [INFO]: Train Epoch: 2, 1520/2646 (57%), Loss: 6.823611\n",
            "INFO:20:Train Epoch: 2, 1520/2646 (57%), Loss: 6.823611\n",
            "2022-12-14 03:04:09,789 [INFO]: Train Epoch: 2, 1540/2646 (58%), Loss: 6.525720\n",
            "INFO:20:Train Epoch: 2, 1540/2646 (58%), Loss: 6.525720\n",
            "2022-12-14 03:04:12,601 [INFO]: Train Epoch: 2, 1560/2646 (59%), Loss: 6.737375\n",
            "INFO:20:Train Epoch: 2, 1560/2646 (59%), Loss: 6.737375\n",
            "2022-12-14 03:04:15,540 [INFO]: Train Epoch: 2, 1580/2646 (60%), Loss: 6.705677\n",
            "INFO:20:Train Epoch: 2, 1580/2646 (60%), Loss: 6.705677\n",
            "2022-12-14 03:04:18,334 [INFO]: Train Epoch: 2, 1600/2646 (60%), Loss: 6.614542\n",
            "INFO:20:Train Epoch: 2, 1600/2646 (60%), Loss: 6.614542\n",
            "2022-12-14 03:04:21,126 [INFO]: Train Epoch: 2, 1620/2646 (61%), Loss: 6.712808\n",
            "INFO:20:Train Epoch: 2, 1620/2646 (61%), Loss: 6.712808\n",
            "2022-12-14 03:04:23,923 [INFO]: Train Epoch: 2, 1640/2646 (62%), Loss: 6.663577\n",
            "INFO:20:Train Epoch: 2, 1640/2646 (62%), Loss: 6.663577\n",
            "2022-12-14 03:04:26,833 [INFO]: Train Epoch: 2, 1660/2646 (63%), Loss: 6.607067\n",
            "INFO:20:Train Epoch: 2, 1660/2646 (63%), Loss: 6.607067\n",
            "2022-12-14 03:04:29,694 [INFO]: Train Epoch: 2, 1680/2646 (63%), Loss: 6.715418\n",
            "INFO:20:Train Epoch: 2, 1680/2646 (63%), Loss: 6.715418\n",
            "2022-12-14 03:04:32,423 [INFO]: Train Epoch: 2, 1700/2646 (64%), Loss: 6.627470\n",
            "INFO:20:Train Epoch: 2, 1700/2646 (64%), Loss: 6.627470\n",
            "2022-12-14 03:04:35,229 [INFO]: Train Epoch: 2, 1720/2646 (65%), Loss: 6.827614\n",
            "INFO:20:Train Epoch: 2, 1720/2646 (65%), Loss: 6.827614\n",
            "2022-12-14 03:04:38,192 [INFO]: Train Epoch: 2, 1740/2646 (66%), Loss: 6.604050\n",
            "INFO:20:Train Epoch: 2, 1740/2646 (66%), Loss: 6.604050\n",
            "2022-12-14 03:04:41,035 [INFO]: Train Epoch: 2, 1760/2646 (67%), Loss: 6.638705\n",
            "INFO:20:Train Epoch: 2, 1760/2646 (67%), Loss: 6.638705\n",
            "2022-12-14 03:04:43,992 [INFO]: Train Epoch: 2, 1780/2646 (67%), Loss: 6.550807\n",
            "INFO:20:Train Epoch: 2, 1780/2646 (67%), Loss: 6.550807\n",
            "2022-12-14 03:04:46,887 [INFO]: Train Epoch: 2, 1800/2646 (68%), Loss: 6.695391\n",
            "INFO:20:Train Epoch: 2, 1800/2646 (68%), Loss: 6.695391\n",
            "2022-12-14 03:04:49,656 [INFO]: Train Epoch: 2, 1820/2646 (69%), Loss: 6.670910\n",
            "INFO:20:Train Epoch: 2, 1820/2646 (69%), Loss: 6.670910\n",
            "2022-12-14 03:04:52,572 [INFO]: Train Epoch: 2, 1840/2646 (70%), Loss: 6.565325\n",
            "INFO:20:Train Epoch: 2, 1840/2646 (70%), Loss: 6.565325\n",
            "2022-12-14 03:04:55,528 [INFO]: Train Epoch: 2, 1860/2646 (70%), Loss: 6.572062\n",
            "INFO:20:Train Epoch: 2, 1860/2646 (70%), Loss: 6.572062\n",
            "2022-12-14 03:04:58,212 [INFO]: Train Epoch: 2, 1880/2646 (71%), Loss: 6.681842\n",
            "INFO:20:Train Epoch: 2, 1880/2646 (71%), Loss: 6.681842\n",
            "2022-12-14 03:05:00,910 [INFO]: Train Epoch: 2, 1900/2646 (72%), Loss: 6.598898\n",
            "INFO:20:Train Epoch: 2, 1900/2646 (72%), Loss: 6.598898\n",
            "2022-12-14 03:05:03,866 [INFO]: Train Epoch: 2, 1920/2646 (73%), Loss: 6.643700\n",
            "INFO:20:Train Epoch: 2, 1920/2646 (73%), Loss: 6.643700\n",
            "2022-12-14 03:05:06,626 [INFO]: Train Epoch: 2, 1940/2646 (73%), Loss: 6.645896\n",
            "INFO:20:Train Epoch: 2, 1940/2646 (73%), Loss: 6.645896\n",
            "2022-12-14 03:05:09,388 [INFO]: Train Epoch: 2, 1960/2646 (74%), Loss: 6.566644\n",
            "INFO:20:Train Epoch: 2, 1960/2646 (74%), Loss: 6.566644\n",
            "2022-12-14 03:05:12,408 [INFO]: Train Epoch: 2, 1980/2646 (75%), Loss: 6.656137\n",
            "INFO:20:Train Epoch: 2, 1980/2646 (75%), Loss: 6.656137\n",
            "2022-12-14 03:05:15,185 [INFO]: Train Epoch: 2, 2000/2646 (76%), Loss: 6.707726\n",
            "INFO:20:Train Epoch: 2, 2000/2646 (76%), Loss: 6.707726\n",
            "2022-12-14 03:05:18,012 [INFO]: Train Epoch: 2, 2020/2646 (76%), Loss: 6.747967\n",
            "INFO:20:Train Epoch: 2, 2020/2646 (76%), Loss: 6.747967\n",
            "2022-12-14 03:05:20,801 [INFO]: Train Epoch: 2, 2040/2646 (77%), Loss: 6.587041\n",
            "INFO:20:Train Epoch: 2, 2040/2646 (77%), Loss: 6.587041\n",
            "2022-12-14 03:05:23,735 [INFO]: Train Epoch: 2, 2060/2646 (78%), Loss: 6.567954\n",
            "INFO:20:Train Epoch: 2, 2060/2646 (78%), Loss: 6.567954\n",
            "2022-12-14 03:05:26,519 [INFO]: Train Epoch: 2, 2080/2646 (79%), Loss: 6.777658\n",
            "INFO:20:Train Epoch: 2, 2080/2646 (79%), Loss: 6.777658\n",
            "2022-12-14 03:05:29,285 [INFO]: Train Epoch: 2, 2100/2646 (79%), Loss: 6.676569\n",
            "INFO:20:Train Epoch: 2, 2100/2646 (79%), Loss: 6.676569\n",
            "2022-12-14 03:05:32,169 [INFO]: Train Epoch: 2, 2120/2646 (80%), Loss: 6.652968\n",
            "INFO:20:Train Epoch: 2, 2120/2646 (80%), Loss: 6.652968\n",
            "2022-12-14 03:05:35,186 [INFO]: Train Epoch: 2, 2140/2646 (81%), Loss: 6.612326\n",
            "INFO:20:Train Epoch: 2, 2140/2646 (81%), Loss: 6.612326\n",
            "2022-12-14 03:05:37,954 [INFO]: Train Epoch: 2, 2160/2646 (82%), Loss: 6.515061\n",
            "INFO:20:Train Epoch: 2, 2160/2646 (82%), Loss: 6.515061\n",
            "2022-12-14 03:05:40,732 [INFO]: Train Epoch: 2, 2180/2646 (82%), Loss: 6.677744\n",
            "INFO:20:Train Epoch: 2, 2180/2646 (82%), Loss: 6.677744\n",
            "2022-12-14 03:05:43,567 [INFO]: Train Epoch: 2, 2200/2646 (83%), Loss: 6.688723\n",
            "INFO:20:Train Epoch: 2, 2200/2646 (83%), Loss: 6.688723\n",
            "2022-12-14 03:05:46,311 [INFO]: Train Epoch: 2, 2220/2646 (84%), Loss: 6.697627\n",
            "INFO:20:Train Epoch: 2, 2220/2646 (84%), Loss: 6.697627\n",
            "2022-12-14 03:05:49,093 [INFO]: Train Epoch: 2, 2240/2646 (85%), Loss: 6.717638\n",
            "INFO:20:Train Epoch: 2, 2240/2646 (85%), Loss: 6.717638\n",
            "2022-12-14 03:05:51,890 [INFO]: Train Epoch: 2, 2260/2646 (85%), Loss: 6.611935\n",
            "INFO:20:Train Epoch: 2, 2260/2646 (85%), Loss: 6.611935\n",
            "2022-12-14 03:05:54,706 [INFO]: Train Epoch: 2, 2280/2646 (86%), Loss: 6.631641\n",
            "INFO:20:Train Epoch: 2, 2280/2646 (86%), Loss: 6.631641\n",
            "2022-12-14 03:05:57,694 [INFO]: Train Epoch: 2, 2300/2646 (87%), Loss: 6.616605\n",
            "INFO:20:Train Epoch: 2, 2300/2646 (87%), Loss: 6.616605\n",
            "2022-12-14 03:06:00,532 [INFO]: Train Epoch: 2, 2320/2646 (88%), Loss: 6.633241\n",
            "INFO:20:Train Epoch: 2, 2320/2646 (88%), Loss: 6.633241\n",
            "2022-12-14 03:06:03,190 [INFO]: Train Epoch: 2, 2340/2646 (88%), Loss: 6.594185\n",
            "INFO:20:Train Epoch: 2, 2340/2646 (88%), Loss: 6.594185\n",
            "2022-12-14 03:06:05,973 [INFO]: Train Epoch: 2, 2360/2646 (89%), Loss: 6.607587\n",
            "INFO:20:Train Epoch: 2, 2360/2646 (89%), Loss: 6.607587\n",
            "2022-12-14 03:06:08,806 [INFO]: Train Epoch: 2, 2380/2646 (90%), Loss: 6.640260\n",
            "INFO:20:Train Epoch: 2, 2380/2646 (90%), Loss: 6.640260\n",
            "2022-12-14 03:06:11,633 [INFO]: Train Epoch: 2, 2400/2646 (91%), Loss: 6.703928\n",
            "INFO:20:Train Epoch: 2, 2400/2646 (91%), Loss: 6.703928\n",
            "2022-12-14 03:06:14,662 [INFO]: Train Epoch: 2, 2420/2646 (91%), Loss: 6.624793\n",
            "INFO:20:Train Epoch: 2, 2420/2646 (91%), Loss: 6.624793\n",
            "2022-12-14 03:06:17,626 [INFO]: Train Epoch: 2, 2440/2646 (92%), Loss: 6.692106\n",
            "INFO:20:Train Epoch: 2, 2440/2646 (92%), Loss: 6.692106\n",
            "2022-12-14 03:06:20,444 [INFO]: Train Epoch: 2, 2460/2646 (93%), Loss: 6.703788\n",
            "INFO:20:Train Epoch: 2, 2460/2646 (93%), Loss: 6.703788\n",
            "2022-12-14 03:06:23,202 [INFO]: Train Epoch: 2, 2480/2646 (94%), Loss: 6.685273\n",
            "INFO:20:Train Epoch: 2, 2480/2646 (94%), Loss: 6.685273\n",
            "2022-12-14 03:06:26,081 [INFO]: Train Epoch: 2, 2500/2646 (94%), Loss: 6.653826\n",
            "INFO:20:Train Epoch: 2, 2500/2646 (94%), Loss: 6.653826\n",
            "2022-12-14 03:06:28,864 [INFO]: Train Epoch: 2, 2520/2646 (95%), Loss: 6.673696\n",
            "INFO:20:Train Epoch: 2, 2520/2646 (95%), Loss: 6.673696\n",
            "2022-12-14 03:06:31,785 [INFO]: Train Epoch: 2, 2540/2646 (96%), Loss: 6.627232\n",
            "INFO:20:Train Epoch: 2, 2540/2646 (96%), Loss: 6.627232\n",
            "2022-12-14 03:06:34,729 [INFO]: Train Epoch: 2, 2560/2646 (97%), Loss: 6.609703\n",
            "INFO:20:Train Epoch: 2, 2560/2646 (97%), Loss: 6.609703\n",
            "2022-12-14 03:06:37,564 [INFO]: Train Epoch: 2, 2580/2646 (98%), Loss: 6.606965\n",
            "INFO:20:Train Epoch: 2, 2580/2646 (98%), Loss: 6.606965\n",
            "2022-12-14 03:06:40,421 [INFO]: Train Epoch: 2, 2600/2646 (98%), Loss: 6.833176\n",
            "INFO:20:Train Epoch: 2, 2600/2646 (98%), Loss: 6.833176\n",
            "2022-12-14 03:06:43,227 [INFO]: Train Epoch: 2, 2620/2646 (99%), Loss: 6.716876\n",
            "INFO:20:Train Epoch: 2, 2620/2646 (99%), Loss: 6.716876\n",
            "2022-12-14 03:06:46,069 [INFO]: Train Epoch: 2, 2640/2646 (100%), Loss: 6.646024\n",
            "INFO:20:Train Epoch: 2, 2640/2646 (100%), Loss: 6.646024\n",
            "2022-12-14 03:06:46,614 [INFO]: Train Epoch: 2, total Loss: 17555.264799, mean Loss: 6.634643\n",
            "INFO:20:Train Epoch: 2, total Loss: 17555.264799, mean Loss: 6.634643\n",
            "2022-12-14 03:06:46,614 [DEBUG]: start validation\n",
            "DEBUG:10:start validation\n",
            "2022-12-14 03:06:49,196 [INFO]: Train Epoch: 2, validation loss is : 7.557\n",
            "INFO:20:Train Epoch: 2, validation loss is : 7.557\n",
            "2022-12-14 03:06:49,196 [DEBUG]: EarlyStopping counter:1 out of 10\n",
            "DEBUG:10:EarlyStopping counter:1 out of 10\n",
            "2022-12-14 03:06:49,197 [DEBUG]: #######: 3 ...\n",
            "DEBUG:10:#######: 3 ...\n",
            "2022-12-14 03:06:49,338 [INFO]: Train Epoch: 3, 0/2646 (0%), Loss: 6.714731\n",
            "INFO:20:Train Epoch: 3, 0/2646 (0%), Loss: 6.714731\n",
            "2022-12-14 03:06:52,031 [INFO]: Train Epoch: 3, 20/2646 (1%), Loss: 6.586408\n",
            "INFO:20:Train Epoch: 3, 20/2646 (1%), Loss: 6.586408\n",
            "2022-12-14 03:06:54,818 [INFO]: Train Epoch: 3, 40/2646 (2%), Loss: 6.704937\n",
            "INFO:20:Train Epoch: 3, 40/2646 (2%), Loss: 6.704937\n",
            "2022-12-14 03:06:57,555 [INFO]: Train Epoch: 3, 60/2646 (2%), Loss: 6.598492\n",
            "INFO:20:Train Epoch: 3, 60/2646 (2%), Loss: 6.598492\n",
            "2022-12-14 03:07:00,397 [INFO]: Train Epoch: 3, 80/2646 (3%), Loss: 6.779705\n",
            "INFO:20:Train Epoch: 3, 80/2646 (3%), Loss: 6.779705\n",
            "2022-12-14 03:07:03,147 [INFO]: Train Epoch: 3, 100/2646 (4%), Loss: 6.772338\n",
            "INFO:20:Train Epoch: 3, 100/2646 (4%), Loss: 6.772338\n",
            "2022-12-14 03:07:05,955 [INFO]: Train Epoch: 3, 120/2646 (5%), Loss: 6.709585\n",
            "INFO:20:Train Epoch: 3, 120/2646 (5%), Loss: 6.709585\n",
            "2022-12-14 03:07:08,685 [INFO]: Train Epoch: 3, 140/2646 (5%), Loss: 6.803205\n",
            "INFO:20:Train Epoch: 3, 140/2646 (5%), Loss: 6.803205\n",
            "2022-12-14 03:07:11,489 [INFO]: Train Epoch: 3, 160/2646 (6%), Loss: 6.675261\n",
            "INFO:20:Train Epoch: 3, 160/2646 (6%), Loss: 6.675261\n",
            "2022-12-14 03:07:14,157 [INFO]: Train Epoch: 3, 180/2646 (7%), Loss: 6.635058\n",
            "INFO:20:Train Epoch: 3, 180/2646 (7%), Loss: 6.635058\n",
            "2022-12-14 03:07:16,978 [INFO]: Train Epoch: 3, 200/2646 (8%), Loss: 6.693959\n",
            "INFO:20:Train Epoch: 3, 200/2646 (8%), Loss: 6.693959\n",
            "2022-12-14 03:07:19,947 [INFO]: Train Epoch: 3, 220/2646 (8%), Loss: 6.698277\n",
            "INFO:20:Train Epoch: 3, 220/2646 (8%), Loss: 6.698277\n",
            "2022-12-14 03:07:22,805 [INFO]: Train Epoch: 3, 240/2646 (9%), Loss: 6.745788\n",
            "INFO:20:Train Epoch: 3, 240/2646 (9%), Loss: 6.745788\n",
            "2022-12-14 03:07:25,532 [INFO]: Train Epoch: 3, 260/2646 (10%), Loss: 6.666989\n",
            "INFO:20:Train Epoch: 3, 260/2646 (10%), Loss: 6.666989\n",
            "2022-12-14 03:07:28,262 [INFO]: Train Epoch: 3, 280/2646 (11%), Loss: 6.648353\n",
            "INFO:20:Train Epoch: 3, 280/2646 (11%), Loss: 6.648353\n",
            "2022-12-14 03:07:31,027 [INFO]: Train Epoch: 3, 300/2646 (11%), Loss: 6.664601\n",
            "INFO:20:Train Epoch: 3, 300/2646 (11%), Loss: 6.664601\n",
            "2022-12-14 03:07:33,751 [INFO]: Train Epoch: 3, 320/2646 (12%), Loss: 6.794031\n",
            "INFO:20:Train Epoch: 3, 320/2646 (12%), Loss: 6.794031\n",
            "2022-12-14 03:07:36,545 [INFO]: Train Epoch: 3, 340/2646 (13%), Loss: 6.775630\n",
            "INFO:20:Train Epoch: 3, 340/2646 (13%), Loss: 6.775630\n",
            "2022-12-14 03:07:39,565 [INFO]: Train Epoch: 3, 360/2646 (14%), Loss: 6.622552\n",
            "INFO:20:Train Epoch: 3, 360/2646 (14%), Loss: 6.622552\n",
            "2022-12-14 03:07:42,390 [INFO]: Train Epoch: 3, 380/2646 (14%), Loss: 6.624461\n",
            "INFO:20:Train Epoch: 3, 380/2646 (14%), Loss: 6.624461\n",
            "2022-12-14 03:07:45,258 [INFO]: Train Epoch: 3, 400/2646 (15%), Loss: 6.766055\n",
            "INFO:20:Train Epoch: 3, 400/2646 (15%), Loss: 6.766055\n",
            "2022-12-14 03:07:48,156 [INFO]: Train Epoch: 3, 420/2646 (16%), Loss: 6.505574\n",
            "INFO:20:Train Epoch: 3, 420/2646 (16%), Loss: 6.505574\n",
            "2022-12-14 03:07:50,997 [INFO]: Train Epoch: 3, 440/2646 (17%), Loss: 6.656321\n",
            "INFO:20:Train Epoch: 3, 440/2646 (17%), Loss: 6.656321\n",
            "2022-12-14 03:07:53,731 [INFO]: Train Epoch: 3, 460/2646 (17%), Loss: 6.704856\n",
            "INFO:20:Train Epoch: 3, 460/2646 (17%), Loss: 6.704856\n",
            "2022-12-14 03:07:56,776 [INFO]: Train Epoch: 3, 480/2646 (18%), Loss: 6.677579\n",
            "INFO:20:Train Epoch: 3, 480/2646 (18%), Loss: 6.677579\n",
            "2022-12-14 03:07:59,654 [INFO]: Train Epoch: 3, 500/2646 (19%), Loss: 6.615087\n",
            "INFO:20:Train Epoch: 3, 500/2646 (19%), Loss: 6.615087\n",
            "2022-12-14 03:08:02,733 [INFO]: Train Epoch: 3, 520/2646 (20%), Loss: 6.570822\n",
            "INFO:20:Train Epoch: 3, 520/2646 (20%), Loss: 6.570822\n",
            "2022-12-14 03:08:05,692 [INFO]: Train Epoch: 3, 540/2646 (20%), Loss: 6.703937\n",
            "INFO:20:Train Epoch: 3, 540/2646 (20%), Loss: 6.703937\n",
            "2022-12-14 03:08:08,664 [INFO]: Train Epoch: 3, 560/2646 (21%), Loss: 6.675433\n",
            "INFO:20:Train Epoch: 3, 560/2646 (21%), Loss: 6.675433\n",
            "2022-12-14 03:08:11,642 [INFO]: Train Epoch: 3, 580/2646 (22%), Loss: 6.671847\n",
            "INFO:20:Train Epoch: 3, 580/2646 (22%), Loss: 6.671847\n",
            "2022-12-14 03:08:14,453 [INFO]: Train Epoch: 3, 600/2646 (23%), Loss: 6.574815\n",
            "INFO:20:Train Epoch: 3, 600/2646 (23%), Loss: 6.574815\n",
            "2022-12-14 03:08:17,316 [INFO]: Train Epoch: 3, 620/2646 (23%), Loss: 6.556647\n",
            "INFO:20:Train Epoch: 3, 620/2646 (23%), Loss: 6.556647\n",
            "2022-12-14 03:08:20,104 [INFO]: Train Epoch: 3, 640/2646 (24%), Loss: 6.571899\n",
            "INFO:20:Train Epoch: 3, 640/2646 (24%), Loss: 6.571899\n",
            "2022-12-14 03:08:22,926 [INFO]: Train Epoch: 3, 660/2646 (25%), Loss: 6.623642\n",
            "INFO:20:Train Epoch: 3, 660/2646 (25%), Loss: 6.623642\n",
            "2022-12-14 03:08:25,695 [INFO]: Train Epoch: 3, 680/2646 (26%), Loss: 6.766645\n",
            "INFO:20:Train Epoch: 3, 680/2646 (26%), Loss: 6.766645\n",
            "2022-12-14 03:08:28,449 [INFO]: Train Epoch: 3, 700/2646 (26%), Loss: 6.605767\n",
            "INFO:20:Train Epoch: 3, 700/2646 (26%), Loss: 6.605767\n",
            "2022-12-14 03:08:31,125 [INFO]: Train Epoch: 3, 720/2646 (27%), Loss: 6.700068\n",
            "INFO:20:Train Epoch: 3, 720/2646 (27%), Loss: 6.700068\n",
            "2022-12-14 03:08:33,948 [INFO]: Train Epoch: 3, 740/2646 (28%), Loss: 6.583791\n",
            "INFO:20:Train Epoch: 3, 740/2646 (28%), Loss: 6.583791\n",
            "2022-12-14 03:08:36,762 [INFO]: Train Epoch: 3, 760/2646 (29%), Loss: 6.708881\n",
            "INFO:20:Train Epoch: 3, 760/2646 (29%), Loss: 6.708881\n",
            "2022-12-14 03:08:39,588 [INFO]: Train Epoch: 3, 780/2646 (29%), Loss: 6.709006\n",
            "INFO:20:Train Epoch: 3, 780/2646 (29%), Loss: 6.709006\n",
            "2022-12-14 03:08:42,531 [INFO]: Train Epoch: 3, 800/2646 (30%), Loss: 6.683836\n",
            "INFO:20:Train Epoch: 3, 800/2646 (30%), Loss: 6.683836\n",
            "2022-12-14 03:08:45,363 [INFO]: Train Epoch: 3, 820/2646 (31%), Loss: 6.693590\n",
            "INFO:20:Train Epoch: 3, 820/2646 (31%), Loss: 6.693590\n",
            "2022-12-14 03:08:48,076 [INFO]: Train Epoch: 3, 840/2646 (32%), Loss: 6.652185\n",
            "INFO:20:Train Epoch: 3, 840/2646 (32%), Loss: 6.652185\n",
            "2022-12-14 03:08:50,849 [INFO]: Train Epoch: 3, 860/2646 (33%), Loss: 6.581814\n",
            "INFO:20:Train Epoch: 3, 860/2646 (33%), Loss: 6.581814\n",
            "2022-12-14 03:08:53,654 [INFO]: Train Epoch: 3, 880/2646 (33%), Loss: 6.693346\n",
            "INFO:20:Train Epoch: 3, 880/2646 (33%), Loss: 6.693346\n",
            "2022-12-14 03:08:56,533 [INFO]: Train Epoch: 3, 900/2646 (34%), Loss: 6.678305\n",
            "INFO:20:Train Epoch: 3, 900/2646 (34%), Loss: 6.678305\n",
            "2022-12-14 03:08:59,448 [INFO]: Train Epoch: 3, 920/2646 (35%), Loss: 6.689573\n",
            "INFO:20:Train Epoch: 3, 920/2646 (35%), Loss: 6.689573\n",
            "2022-12-14 03:09:02,285 [INFO]: Train Epoch: 3, 940/2646 (36%), Loss: 6.590291\n",
            "INFO:20:Train Epoch: 3, 940/2646 (36%), Loss: 6.590291\n",
            "2022-12-14 03:09:05,087 [INFO]: Train Epoch: 3, 960/2646 (36%), Loss: 6.814870\n",
            "INFO:20:Train Epoch: 3, 960/2646 (36%), Loss: 6.814870\n",
            "2022-12-14 03:09:08,029 [INFO]: Train Epoch: 3, 980/2646 (37%), Loss: 6.709976\n",
            "INFO:20:Train Epoch: 3, 980/2646 (37%), Loss: 6.709976\n",
            "2022-12-14 03:09:10,827 [INFO]: Train Epoch: 3, 1000/2646 (38%), Loss: 6.731426\n",
            "INFO:20:Train Epoch: 3, 1000/2646 (38%), Loss: 6.731426\n",
            "2022-12-14 03:09:13,669 [INFO]: Train Epoch: 3, 1020/2646 (39%), Loss: 6.605369\n",
            "INFO:20:Train Epoch: 3, 1020/2646 (39%), Loss: 6.605369\n",
            "2022-12-14 03:09:16,432 [INFO]: Train Epoch: 3, 1040/2646 (39%), Loss: 6.727530\n",
            "INFO:20:Train Epoch: 3, 1040/2646 (39%), Loss: 6.727530\n",
            "2022-12-14 03:09:19,175 [INFO]: Train Epoch: 3, 1060/2646 (40%), Loss: 6.658712\n",
            "INFO:20:Train Epoch: 3, 1060/2646 (40%), Loss: 6.658712\n",
            "2022-12-14 03:09:21,801 [INFO]: Train Epoch: 3, 1080/2646 (41%), Loss: 6.666625\n",
            "INFO:20:Train Epoch: 3, 1080/2646 (41%), Loss: 6.666625\n",
            "2022-12-14 03:09:24,553 [INFO]: Train Epoch: 3, 1100/2646 (42%), Loss: 6.742220\n",
            "INFO:20:Train Epoch: 3, 1100/2646 (42%), Loss: 6.742220\n",
            "2022-12-14 03:09:27,345 [INFO]: Train Epoch: 3, 1120/2646 (42%), Loss: 6.702367\n",
            "INFO:20:Train Epoch: 3, 1120/2646 (42%), Loss: 6.702367\n",
            "2022-12-14 03:09:30,100 [INFO]: Train Epoch: 3, 1140/2646 (43%), Loss: 6.654949\n",
            "INFO:20:Train Epoch: 3, 1140/2646 (43%), Loss: 6.654949\n",
            "2022-12-14 03:09:33,005 [INFO]: Train Epoch: 3, 1160/2646 (44%), Loss: 6.629869\n",
            "INFO:20:Train Epoch: 3, 1160/2646 (44%), Loss: 6.629869\n",
            "2022-12-14 03:09:35,847 [INFO]: Train Epoch: 3, 1180/2646 (45%), Loss: 6.594645\n",
            "INFO:20:Train Epoch: 3, 1180/2646 (45%), Loss: 6.594645\n",
            "2022-12-14 03:09:38,726 [INFO]: Train Epoch: 3, 1200/2646 (45%), Loss: 6.648673\n",
            "INFO:20:Train Epoch: 3, 1200/2646 (45%), Loss: 6.648673\n",
            "2022-12-14 03:09:41,401 [INFO]: Train Epoch: 3, 1220/2646 (46%), Loss: 6.497078\n",
            "INFO:20:Train Epoch: 3, 1220/2646 (46%), Loss: 6.497078\n",
            "2022-12-14 03:09:44,227 [INFO]: Train Epoch: 3, 1240/2646 (47%), Loss: 6.751645\n",
            "INFO:20:Train Epoch: 3, 1240/2646 (47%), Loss: 6.751645\n",
            "2022-12-14 03:09:46,934 [INFO]: Train Epoch: 3, 1260/2646 (48%), Loss: 6.691204\n",
            "INFO:20:Train Epoch: 3, 1260/2646 (48%), Loss: 6.691204\n",
            "2022-12-14 03:09:49,884 [INFO]: Train Epoch: 3, 1280/2646 (48%), Loss: 6.584896\n",
            "INFO:20:Train Epoch: 3, 1280/2646 (48%), Loss: 6.584896\n",
            "2022-12-14 03:09:52,687 [INFO]: Train Epoch: 3, 1300/2646 (49%), Loss: 6.633612\n",
            "INFO:20:Train Epoch: 3, 1300/2646 (49%), Loss: 6.633612\n",
            "2022-12-14 03:09:55,523 [INFO]: Train Epoch: 3, 1320/2646 (50%), Loss: 6.605583\n",
            "INFO:20:Train Epoch: 3, 1320/2646 (50%), Loss: 6.605583\n",
            "2022-12-14 03:09:58,371 [INFO]: Train Epoch: 3, 1340/2646 (51%), Loss: 6.634448\n",
            "INFO:20:Train Epoch: 3, 1340/2646 (51%), Loss: 6.634448\n",
            "2022-12-14 03:10:01,234 [INFO]: Train Epoch: 3, 1360/2646 (51%), Loss: 6.661785\n",
            "INFO:20:Train Epoch: 3, 1360/2646 (51%), Loss: 6.661785\n",
            "2022-12-14 03:10:03,950 [INFO]: Train Epoch: 3, 1380/2646 (52%), Loss: 6.564334\n",
            "INFO:20:Train Epoch: 3, 1380/2646 (52%), Loss: 6.564334\n",
            "2022-12-14 03:10:06,947 [INFO]: Train Epoch: 3, 1400/2646 (53%), Loss: 6.626279\n",
            "INFO:20:Train Epoch: 3, 1400/2646 (53%), Loss: 6.626279\n",
            "2022-12-14 03:10:09,763 [INFO]: Train Epoch: 3, 1420/2646 (54%), Loss: 6.632476\n",
            "INFO:20:Train Epoch: 3, 1420/2646 (54%), Loss: 6.632476\n",
            "2022-12-14 03:10:12,697 [INFO]: Train Epoch: 3, 1440/2646 (54%), Loss: 6.702673\n",
            "INFO:20:Train Epoch: 3, 1440/2646 (54%), Loss: 6.702673\n",
            "2022-12-14 03:10:15,493 [INFO]: Train Epoch: 3, 1460/2646 (55%), Loss: 6.810952\n",
            "INFO:20:Train Epoch: 3, 1460/2646 (55%), Loss: 6.810952\n",
            "2022-12-14 03:10:18,291 [INFO]: Train Epoch: 3, 1480/2646 (56%), Loss: 6.674837\n",
            "INFO:20:Train Epoch: 3, 1480/2646 (56%), Loss: 6.674837\n",
            "2022-12-14 03:10:21,113 [INFO]: Train Epoch: 3, 1500/2646 (57%), Loss: 6.662167\n",
            "INFO:20:Train Epoch: 3, 1500/2646 (57%), Loss: 6.662167\n",
            "2022-12-14 03:10:24,023 [INFO]: Train Epoch: 3, 1520/2646 (57%), Loss: 6.860474\n",
            "INFO:20:Train Epoch: 3, 1520/2646 (57%), Loss: 6.860474\n",
            "2022-12-14 03:10:26,943 [INFO]: Train Epoch: 3, 1540/2646 (58%), Loss: 6.545553\n",
            "INFO:20:Train Epoch: 3, 1540/2646 (58%), Loss: 6.545553\n",
            "2022-12-14 03:10:29,740 [INFO]: Train Epoch: 3, 1560/2646 (59%), Loss: 6.679753\n",
            "INFO:20:Train Epoch: 3, 1560/2646 (59%), Loss: 6.679753\n",
            "2022-12-14 03:10:32,679 [INFO]: Train Epoch: 3, 1580/2646 (60%), Loss: 6.677177\n",
            "INFO:20:Train Epoch: 3, 1580/2646 (60%), Loss: 6.677177\n",
            "2022-12-14 03:10:35,467 [INFO]: Train Epoch: 3, 1600/2646 (60%), Loss: 6.598165\n",
            "INFO:20:Train Epoch: 3, 1600/2646 (60%), Loss: 6.598165\n",
            "2022-12-14 03:10:38,271 [INFO]: Train Epoch: 3, 1620/2646 (61%), Loss: 6.690819\n",
            "INFO:20:Train Epoch: 3, 1620/2646 (61%), Loss: 6.690819\n",
            "2022-12-14 03:10:41,056 [INFO]: Train Epoch: 3, 1640/2646 (62%), Loss: 6.663703\n",
            "INFO:20:Train Epoch: 3, 1640/2646 (62%), Loss: 6.663703\n",
            "2022-12-14 03:10:43,951 [INFO]: Train Epoch: 3, 1660/2646 (63%), Loss: 6.640791\n",
            "INFO:20:Train Epoch: 3, 1660/2646 (63%), Loss: 6.640791\n",
            "2022-12-14 03:10:46,792 [INFO]: Train Epoch: 3, 1680/2646 (63%), Loss: 6.720922\n",
            "INFO:20:Train Epoch: 3, 1680/2646 (63%), Loss: 6.720922\n",
            "2022-12-14 03:10:49,504 [INFO]: Train Epoch: 3, 1700/2646 (64%), Loss: 6.615221\n",
            "INFO:20:Train Epoch: 3, 1700/2646 (64%), Loss: 6.615221\n",
            "2022-12-14 03:10:52,303 [INFO]: Train Epoch: 3, 1720/2646 (65%), Loss: 6.828566\n",
            "INFO:20:Train Epoch: 3, 1720/2646 (65%), Loss: 6.828566\n",
            "2022-12-14 03:10:55,249 [INFO]: Train Epoch: 3, 1740/2646 (66%), Loss: 6.611227\n",
            "INFO:20:Train Epoch: 3, 1740/2646 (66%), Loss: 6.611227\n",
            "2022-12-14 03:10:58,086 [INFO]: Train Epoch: 3, 1760/2646 (67%), Loss: 6.626684\n",
            "INFO:20:Train Epoch: 3, 1760/2646 (67%), Loss: 6.626684\n",
            "2022-12-14 03:11:01,024 [INFO]: Train Epoch: 3, 1780/2646 (67%), Loss: 6.565159\n",
            "INFO:20:Train Epoch: 3, 1780/2646 (67%), Loss: 6.565159\n",
            "2022-12-14 03:11:03,915 [INFO]: Train Epoch: 3, 1800/2646 (68%), Loss: 6.656299\n",
            "INFO:20:Train Epoch: 3, 1800/2646 (68%), Loss: 6.656299\n",
            "2022-12-14 03:11:06,688 [INFO]: Train Epoch: 3, 1820/2646 (69%), Loss: 6.690595\n",
            "INFO:20:Train Epoch: 3, 1820/2646 (69%), Loss: 6.690595\n",
            "2022-12-14 03:11:09,606 [INFO]: Train Epoch: 3, 1840/2646 (70%), Loss: 6.578962\n",
            "INFO:20:Train Epoch: 3, 1840/2646 (70%), Loss: 6.578962\n",
            "2022-12-14 03:11:12,398 [INFO]: Train Epoch: 3, 1860/2646 (70%), Loss: 6.616844\n",
            "INFO:20:Train Epoch: 3, 1860/2646 (70%), Loss: 6.616844\n",
            "2022-12-14 03:11:15,075 [INFO]: Train Epoch: 3, 1880/2646 (71%), Loss: 6.715377\n",
            "INFO:20:Train Epoch: 3, 1880/2646 (71%), Loss: 6.715377\n",
            "2022-12-14 03:11:17,767 [INFO]: Train Epoch: 3, 1900/2646 (72%), Loss: 6.628695\n",
            "INFO:20:Train Epoch: 3, 1900/2646 (72%), Loss: 6.628695\n",
            "2022-12-14 03:11:20,727 [INFO]: Train Epoch: 3, 1920/2646 (73%), Loss: 6.686660\n",
            "INFO:20:Train Epoch: 3, 1920/2646 (73%), Loss: 6.686660\n",
            "2022-12-14 03:11:23,499 [INFO]: Train Epoch: 3, 1940/2646 (73%), Loss: 6.677076\n",
            "INFO:20:Train Epoch: 3, 1940/2646 (73%), Loss: 6.677076\n",
            "2022-12-14 03:11:26,280 [INFO]: Train Epoch: 3, 1960/2646 (74%), Loss: 6.591939\n",
            "INFO:20:Train Epoch: 3, 1960/2646 (74%), Loss: 6.591939\n",
            "2022-12-14 03:11:29,314 [INFO]: Train Epoch: 3, 1980/2646 (75%), Loss: 6.678378\n",
            "INFO:20:Train Epoch: 3, 1980/2646 (75%), Loss: 6.678378\n",
            "2022-12-14 03:11:32,111 [INFO]: Train Epoch: 3, 2000/2646 (76%), Loss: 6.739066\n",
            "INFO:20:Train Epoch: 3, 2000/2646 (76%), Loss: 6.739066\n",
            "2022-12-14 03:11:34,967 [INFO]: Train Epoch: 3, 2020/2646 (76%), Loss: 6.751765\n",
            "INFO:20:Train Epoch: 3, 2020/2646 (76%), Loss: 6.751765\n",
            "2022-12-14 03:11:37,766 [INFO]: Train Epoch: 3, 2040/2646 (77%), Loss: 6.630057\n",
            "INFO:20:Train Epoch: 3, 2040/2646 (77%), Loss: 6.630057\n",
            "2022-12-14 03:11:40,710 [INFO]: Train Epoch: 3, 2060/2646 (78%), Loss: 6.602124\n",
            "INFO:20:Train Epoch: 3, 2060/2646 (78%), Loss: 6.602124\n",
            "2022-12-14 03:11:43,496 [INFO]: Train Epoch: 3, 2080/2646 (79%), Loss: 6.837246\n",
            "INFO:20:Train Epoch: 3, 2080/2646 (79%), Loss: 6.837246\n",
            "2022-12-14 03:11:46,280 [INFO]: Train Epoch: 3, 2100/2646 (79%), Loss: 6.708525\n",
            "INFO:20:Train Epoch: 3, 2100/2646 (79%), Loss: 6.708525\n",
            "2022-12-14 03:11:49,163 [INFO]: Train Epoch: 3, 2120/2646 (80%), Loss: 6.621464\n",
            "INFO:20:Train Epoch: 3, 2120/2646 (80%), Loss: 6.621464\n",
            "2022-12-14 03:11:52,185 [INFO]: Train Epoch: 3, 2140/2646 (81%), Loss: 6.637138\n",
            "INFO:20:Train Epoch: 3, 2140/2646 (81%), Loss: 6.637138\n",
            "2022-12-14 03:11:54,958 [INFO]: Train Epoch: 3, 2160/2646 (82%), Loss: 6.519632\n",
            "INFO:20:Train Epoch: 3, 2160/2646 (82%), Loss: 6.519632\n",
            "2022-12-14 03:11:57,725 [INFO]: Train Epoch: 3, 2180/2646 (82%), Loss: 6.705255\n",
            "INFO:20:Train Epoch: 3, 2180/2646 (82%), Loss: 6.705255\n",
            "2022-12-14 03:12:00,553 [INFO]: Train Epoch: 3, 2200/2646 (83%), Loss: 6.687103\n",
            "INFO:20:Train Epoch: 3, 2200/2646 (83%), Loss: 6.687103\n",
            "2022-12-14 03:12:03,426 [INFO]: Train Epoch: 3, 2220/2646 (84%), Loss: 6.723166\n",
            "INFO:20:Train Epoch: 3, 2220/2646 (84%), Loss: 6.723166\n",
            "2022-12-14 03:12:06,233 [INFO]: Train Epoch: 3, 2240/2646 (85%), Loss: 6.762751\n",
            "INFO:20:Train Epoch: 3, 2240/2646 (85%), Loss: 6.762751\n",
            "2022-12-14 03:12:09,023 [INFO]: Train Epoch: 3, 2260/2646 (85%), Loss: 6.643757\n",
            "INFO:20:Train Epoch: 3, 2260/2646 (85%), Loss: 6.643757\n",
            "2022-12-14 03:12:11,766 [INFO]: Train Epoch: 3, 2280/2646 (86%), Loss: 6.698318\n",
            "INFO:20:Train Epoch: 3, 2280/2646 (86%), Loss: 6.698318\n",
            "2022-12-14 03:12:14,673 [INFO]: Train Epoch: 3, 2300/2646 (87%), Loss: 6.608269\n",
            "INFO:20:Train Epoch: 3, 2300/2646 (87%), Loss: 6.608269\n",
            "2022-12-14 03:12:17,501 [INFO]: Train Epoch: 3, 2320/2646 (88%), Loss: 6.672798\n",
            "INFO:20:Train Epoch: 3, 2320/2646 (88%), Loss: 6.672798\n",
            "2022-12-14 03:12:20,138 [INFO]: Train Epoch: 3, 2340/2646 (88%), Loss: 6.571813\n",
            "INFO:20:Train Epoch: 3, 2340/2646 (88%), Loss: 6.571813\n",
            "2022-12-14 03:12:22,933 [INFO]: Train Epoch: 3, 2360/2646 (89%), Loss: 6.618249\n",
            "INFO:20:Train Epoch: 3, 2360/2646 (89%), Loss: 6.618249\n",
            "2022-12-14 03:12:25,733 [INFO]: Train Epoch: 3, 2380/2646 (90%), Loss: 6.603323\n",
            "INFO:20:Train Epoch: 3, 2380/2646 (90%), Loss: 6.603323\n",
            "2022-12-14 03:12:28,554 [INFO]: Train Epoch: 3, 2400/2646 (91%), Loss: 6.721498\n",
            "INFO:20:Train Epoch: 3, 2400/2646 (91%), Loss: 6.721498\n",
            "2022-12-14 03:12:31,585 [INFO]: Train Epoch: 3, 2420/2646 (91%), Loss: 6.649949\n",
            "INFO:20:Train Epoch: 3, 2420/2646 (91%), Loss: 6.649949\n",
            "2022-12-14 03:12:34,555 [INFO]: Train Epoch: 3, 2440/2646 (92%), Loss: 6.727400\n",
            "INFO:20:Train Epoch: 3, 2440/2646 (92%), Loss: 6.727400\n",
            "2022-12-14 03:12:37,380 [INFO]: Train Epoch: 3, 2460/2646 (93%), Loss: 6.713298\n",
            "INFO:20:Train Epoch: 3, 2460/2646 (93%), Loss: 6.713298\n",
            "2022-12-14 03:12:40,150 [INFO]: Train Epoch: 3, 2480/2646 (94%), Loss: 6.721460\n",
            "INFO:20:Train Epoch: 3, 2480/2646 (94%), Loss: 6.721460\n",
            "2022-12-14 03:12:43,017 [INFO]: Train Epoch: 3, 2500/2646 (94%), Loss: 6.715374\n",
            "INFO:20:Train Epoch: 3, 2500/2646 (94%), Loss: 6.715374\n",
            "2022-12-14 03:12:45,802 [INFO]: Train Epoch: 3, 2520/2646 (95%), Loss: 6.689600\n",
            "INFO:20:Train Epoch: 3, 2520/2646 (95%), Loss: 6.689600\n",
            "2022-12-14 03:12:48,728 [INFO]: Train Epoch: 3, 2540/2646 (96%), Loss: 6.626446\n",
            "INFO:20:Train Epoch: 3, 2540/2646 (96%), Loss: 6.626446\n",
            "2022-12-14 03:12:51,658 [INFO]: Train Epoch: 3, 2560/2646 (97%), Loss: 6.595796\n",
            "INFO:20:Train Epoch: 3, 2560/2646 (97%), Loss: 6.595796\n",
            "2022-12-14 03:12:54,497 [INFO]: Train Epoch: 3, 2580/2646 (98%), Loss: 6.633132\n",
            "INFO:20:Train Epoch: 3, 2580/2646 (98%), Loss: 6.633132\n",
            "2022-12-14 03:12:57,351 [INFO]: Train Epoch: 3, 2600/2646 (98%), Loss: 6.876227\n",
            "INFO:20:Train Epoch: 3, 2600/2646 (98%), Loss: 6.876227\n",
            "2022-12-14 03:13:00,167 [INFO]: Train Epoch: 3, 2620/2646 (99%), Loss: 6.771911\n",
            "INFO:20:Train Epoch: 3, 2620/2646 (99%), Loss: 6.771911\n",
            "2022-12-14 03:13:03,010 [INFO]: Train Epoch: 3, 2640/2646 (100%), Loss: 6.597610\n",
            "INFO:20:Train Epoch: 3, 2640/2646 (100%), Loss: 6.597610\n",
            "2022-12-14 03:13:03,555 [INFO]: Train Epoch: 3, total Loss: 17639.447730, mean Loss: 6.666458\n",
            "INFO:20:Train Epoch: 3, total Loss: 17639.447730, mean Loss: 6.666458\n",
            "2022-12-14 03:13:03,555 [DEBUG]: start validation\n",
            "DEBUG:10:start validation\n",
            "2022-12-14 03:13:06,133 [INFO]: Train Epoch: 3, validation loss is : 7.547\n",
            "INFO:20:Train Epoch: 3, validation loss is : 7.547\n",
            "2022-12-14 03:13:06,134 [DEBUG]: EarlyStopping counter:2 out of 10\n",
            "DEBUG:10:EarlyStopping counter:2 out of 10\n",
            "2022-12-14 03:13:06,134 [DEBUG]: #######: 4 ...\n",
            "DEBUG:10:#######: 4 ...\n",
            "2022-12-14 03:13:06,273 [INFO]: Train Epoch: 4, 0/2646 (0%), Loss: 6.676643\n",
            "INFO:20:Train Epoch: 4, 0/2646 (0%), Loss: 6.676643\n",
            "2022-12-14 03:13:08,990 [INFO]: Train Epoch: 4, 20/2646 (1%), Loss: 6.604418\n",
            "INFO:20:Train Epoch: 4, 20/2646 (1%), Loss: 6.604418\n",
            "2022-12-14 03:13:11,790 [INFO]: Train Epoch: 4, 40/2646 (2%), Loss: 6.693699\n",
            "INFO:20:Train Epoch: 4, 40/2646 (2%), Loss: 6.693699\n",
            "2022-12-14 03:13:14,531 [INFO]: Train Epoch: 4, 60/2646 (2%), Loss: 6.569208\n",
            "INFO:20:Train Epoch: 4, 60/2646 (2%), Loss: 6.569208\n",
            "2022-12-14 03:13:17,367 [INFO]: Train Epoch: 4, 80/2646 (3%), Loss: 6.743781\n",
            "INFO:20:Train Epoch: 4, 80/2646 (3%), Loss: 6.743781\n",
            "2022-12-14 03:13:20,114 [INFO]: Train Epoch: 4, 100/2646 (4%), Loss: 6.745827\n",
            "INFO:20:Train Epoch: 4, 100/2646 (4%), Loss: 6.745827\n",
            "2022-12-14 03:13:22,924 [INFO]: Train Epoch: 4, 120/2646 (5%), Loss: 6.716011\n",
            "INFO:20:Train Epoch: 4, 120/2646 (5%), Loss: 6.716011\n",
            "2022-12-14 03:13:25,637 [INFO]: Train Epoch: 4, 140/2646 (5%), Loss: 6.802325\n",
            "INFO:20:Train Epoch: 4, 140/2646 (5%), Loss: 6.802325\n",
            "2022-12-14 03:13:28,442 [INFO]: Train Epoch: 4, 160/2646 (6%), Loss: 6.639977\n",
            "INFO:20:Train Epoch: 4, 160/2646 (6%), Loss: 6.639977\n",
            "2022-12-14 03:13:31,107 [INFO]: Train Epoch: 4, 180/2646 (7%), Loss: 6.625632\n",
            "INFO:20:Train Epoch: 4, 180/2646 (7%), Loss: 6.625632\n",
            "2022-12-14 03:13:33,943 [INFO]: Train Epoch: 4, 200/2646 (8%), Loss: 6.676155\n",
            "INFO:20:Train Epoch: 4, 200/2646 (8%), Loss: 6.676155\n",
            "2022-12-14 03:13:36,909 [INFO]: Train Epoch: 4, 220/2646 (8%), Loss: 6.712003\n",
            "INFO:20:Train Epoch: 4, 220/2646 (8%), Loss: 6.712003\n",
            "2022-12-14 03:13:39,751 [INFO]: Train Epoch: 4, 240/2646 (9%), Loss: 6.683818\n",
            "INFO:20:Train Epoch: 4, 240/2646 (9%), Loss: 6.683818\n",
            "2022-12-14 03:13:42,492 [INFO]: Train Epoch: 4, 260/2646 (10%), Loss: 6.635584\n",
            "INFO:20:Train Epoch: 4, 260/2646 (10%), Loss: 6.635584\n",
            "2022-12-14 03:13:45,216 [INFO]: Train Epoch: 4, 280/2646 (11%), Loss: 6.626429\n",
            "INFO:20:Train Epoch: 4, 280/2646 (11%), Loss: 6.626429\n",
            "2022-12-14 03:13:47,976 [INFO]: Train Epoch: 4, 300/2646 (11%), Loss: 6.624945\n",
            "INFO:20:Train Epoch: 4, 300/2646 (11%), Loss: 6.624945\n",
            "2022-12-14 03:13:50,697 [INFO]: Train Epoch: 4, 320/2646 (12%), Loss: 6.801788\n",
            "INFO:20:Train Epoch: 4, 320/2646 (12%), Loss: 6.801788\n",
            "2022-12-14 03:13:53,496 [INFO]: Train Epoch: 4, 340/2646 (13%), Loss: 6.749947\n",
            "INFO:20:Train Epoch: 4, 340/2646 (13%), Loss: 6.749947\n",
            "2022-12-14 03:13:56,513 [INFO]: Train Epoch: 4, 360/2646 (14%), Loss: 6.609069\n",
            "INFO:20:Train Epoch: 4, 360/2646 (14%), Loss: 6.609069\n",
            "2022-12-14 03:13:59,340 [INFO]: Train Epoch: 4, 380/2646 (14%), Loss: 6.652208\n",
            "INFO:20:Train Epoch: 4, 380/2646 (14%), Loss: 6.652208\n",
            "2022-12-14 03:14:02,215 [INFO]: Train Epoch: 4, 400/2646 (15%), Loss: 6.755918\n",
            "INFO:20:Train Epoch: 4, 400/2646 (15%), Loss: 6.755918\n",
            "2022-12-14 03:14:05,104 [INFO]: Train Epoch: 4, 420/2646 (16%), Loss: 6.502154\n",
            "INFO:20:Train Epoch: 4, 420/2646 (16%), Loss: 6.502154\n",
            "2022-12-14 03:14:07,957 [INFO]: Train Epoch: 4, 440/2646 (17%), Loss: 6.640101\n",
            "INFO:20:Train Epoch: 4, 440/2646 (17%), Loss: 6.640101\n",
            "2022-12-14 03:14:10,687 [INFO]: Train Epoch: 4, 460/2646 (17%), Loss: 6.716323\n",
            "INFO:20:Train Epoch: 4, 460/2646 (17%), Loss: 6.716323\n",
            "2022-12-14 03:14:13,726 [INFO]: Train Epoch: 4, 480/2646 (18%), Loss: 6.659708\n",
            "INFO:20:Train Epoch: 4, 480/2646 (18%), Loss: 6.659708\n",
            "2022-12-14 03:14:16,609 [INFO]: Train Epoch: 4, 500/2646 (19%), Loss: 6.620553\n",
            "INFO:20:Train Epoch: 4, 500/2646 (19%), Loss: 6.620553\n",
            "2022-12-14 03:14:19,686 [INFO]: Train Epoch: 4, 520/2646 (20%), Loss: 6.572936\n",
            "INFO:20:Train Epoch: 4, 520/2646 (20%), Loss: 6.572936\n",
            "2022-12-14 03:14:22,641 [INFO]: Train Epoch: 4, 540/2646 (20%), Loss: 6.672028\n",
            "INFO:20:Train Epoch: 4, 540/2646 (20%), Loss: 6.672028\n",
            "2022-12-14 03:14:25,615 [INFO]: Train Epoch: 4, 560/2646 (21%), Loss: 6.686824\n",
            "INFO:20:Train Epoch: 4, 560/2646 (21%), Loss: 6.686824\n",
            "2022-12-14 03:14:28,606 [INFO]: Train Epoch: 4, 580/2646 (22%), Loss: 6.657935\n",
            "INFO:20:Train Epoch: 4, 580/2646 (22%), Loss: 6.657935\n",
            "2022-12-14 03:14:31,436 [INFO]: Train Epoch: 4, 600/2646 (23%), Loss: 6.579093\n",
            "INFO:20:Train Epoch: 4, 600/2646 (23%), Loss: 6.579093\n",
            "2022-12-14 03:14:34,296 [INFO]: Train Epoch: 4, 620/2646 (23%), Loss: 6.602400\n",
            "INFO:20:Train Epoch: 4, 620/2646 (23%), Loss: 6.602400\n",
            "2022-12-14 03:14:37,098 [INFO]: Train Epoch: 4, 640/2646 (24%), Loss: 6.553003\n",
            "INFO:20:Train Epoch: 4, 640/2646 (24%), Loss: 6.553003\n",
            "2022-12-14 03:14:39,928 [INFO]: Train Epoch: 4, 660/2646 (25%), Loss: 6.631586\n",
            "INFO:20:Train Epoch: 4, 660/2646 (25%), Loss: 6.631586\n",
            "2022-12-14 03:14:42,702 [INFO]: Train Epoch: 4, 680/2646 (26%), Loss: 6.745225\n",
            "INFO:20:Train Epoch: 4, 680/2646 (26%), Loss: 6.745225\n",
            "2022-12-14 03:14:45,458 [INFO]: Train Epoch: 4, 700/2646 (26%), Loss: 6.575418\n",
            "INFO:20:Train Epoch: 4, 700/2646 (26%), Loss: 6.575418\n",
            "2022-12-14 03:14:48,150 [INFO]: Train Epoch: 4, 720/2646 (27%), Loss: 6.695465\n",
            "INFO:20:Train Epoch: 4, 720/2646 (27%), Loss: 6.695465\n",
            "2022-12-14 03:14:50,974 [INFO]: Train Epoch: 4, 740/2646 (28%), Loss: 6.559376\n",
            "INFO:20:Train Epoch: 4, 740/2646 (28%), Loss: 6.559376\n",
            "2022-12-14 03:14:53,800 [INFO]: Train Epoch: 4, 760/2646 (29%), Loss: 6.711573\n",
            "INFO:20:Train Epoch: 4, 760/2646 (29%), Loss: 6.711573\n",
            "2022-12-14 03:14:56,619 [INFO]: Train Epoch: 4, 780/2646 (29%), Loss: 6.694639\n",
            "INFO:20:Train Epoch: 4, 780/2646 (29%), Loss: 6.694639\n",
            "2022-12-14 03:14:59,573 [INFO]: Train Epoch: 4, 800/2646 (30%), Loss: 6.699059\n",
            "INFO:20:Train Epoch: 4, 800/2646 (30%), Loss: 6.699059\n",
            "2022-12-14 03:15:02,415 [INFO]: Train Epoch: 4, 820/2646 (31%), Loss: 6.681421\n",
            "INFO:20:Train Epoch: 4, 820/2646 (31%), Loss: 6.681421\n",
            "2022-12-14 03:15:05,164 [INFO]: Train Epoch: 4, 840/2646 (32%), Loss: 6.613309\n",
            "INFO:20:Train Epoch: 4, 840/2646 (32%), Loss: 6.613309\n",
            "2022-12-14 03:15:08,046 [INFO]: Train Epoch: 4, 860/2646 (33%), Loss: 6.587309\n",
            "INFO:20:Train Epoch: 4, 860/2646 (33%), Loss: 6.587309\n",
            "2022-12-14 03:15:10,867 [INFO]: Train Epoch: 4, 880/2646 (33%), Loss: 6.687968\n",
            "INFO:20:Train Epoch: 4, 880/2646 (33%), Loss: 6.687968\n",
            "2022-12-14 03:15:13,703 [INFO]: Train Epoch: 4, 900/2646 (34%), Loss: 6.678394\n",
            "INFO:20:Train Epoch: 4, 900/2646 (34%), Loss: 6.678394\n",
            "2022-12-14 03:15:16,509 [INFO]: Train Epoch: 4, 920/2646 (35%), Loss: 6.667688\n",
            "INFO:20:Train Epoch: 4, 920/2646 (35%), Loss: 6.667688\n",
            "2022-12-14 03:15:19,350 [INFO]: Train Epoch: 4, 940/2646 (36%), Loss: 6.602322\n",
            "INFO:20:Train Epoch: 4, 940/2646 (36%), Loss: 6.602322\n",
            "2022-12-14 03:15:22,157 [INFO]: Train Epoch: 4, 960/2646 (36%), Loss: 6.719665\n",
            "INFO:20:Train Epoch: 4, 960/2646 (36%), Loss: 6.719665\n",
            "2022-12-14 03:15:25,095 [INFO]: Train Epoch: 4, 980/2646 (37%), Loss: 6.692069\n",
            "INFO:20:Train Epoch: 4, 980/2646 (37%), Loss: 6.692069\n",
            "2022-12-14 03:15:27,890 [INFO]: Train Epoch: 4, 1000/2646 (38%), Loss: 6.720916\n",
            "INFO:20:Train Epoch: 4, 1000/2646 (38%), Loss: 6.720916\n",
            "2022-12-14 03:15:30,728 [INFO]: Train Epoch: 4, 1020/2646 (39%), Loss: 6.614283\n",
            "INFO:20:Train Epoch: 4, 1020/2646 (39%), Loss: 6.614283\n",
            "2022-12-14 03:15:33,496 [INFO]: Train Epoch: 4, 1040/2646 (39%), Loss: 6.725179\n",
            "INFO:20:Train Epoch: 4, 1040/2646 (39%), Loss: 6.725179\n",
            "2022-12-14 03:15:36,236 [INFO]: Train Epoch: 4, 1060/2646 (40%), Loss: 6.670959\n",
            "INFO:20:Train Epoch: 4, 1060/2646 (40%), Loss: 6.670959\n",
            "2022-12-14 03:15:38,875 [INFO]: Train Epoch: 4, 1080/2646 (41%), Loss: 6.646841\n",
            "INFO:20:Train Epoch: 4, 1080/2646 (41%), Loss: 6.646841\n",
            "2022-12-14 03:15:41,617 [INFO]: Train Epoch: 4, 1100/2646 (42%), Loss: 6.735379\n",
            "INFO:20:Train Epoch: 4, 1100/2646 (42%), Loss: 6.735379\n",
            "2022-12-14 03:15:44,407 [INFO]: Train Epoch: 4, 1120/2646 (42%), Loss: 6.702682\n",
            "INFO:20:Train Epoch: 4, 1120/2646 (42%), Loss: 6.702682\n",
            "2022-12-14 03:15:47,154 [INFO]: Train Epoch: 4, 1140/2646 (43%), Loss: 6.641902\n",
            "INFO:20:Train Epoch: 4, 1140/2646 (43%), Loss: 6.641902\n",
            "2022-12-14 03:15:50,043 [INFO]: Train Epoch: 4, 1160/2646 (44%), Loss: 6.627291\n",
            "INFO:20:Train Epoch: 4, 1160/2646 (44%), Loss: 6.627291\n",
            "2022-12-14 03:15:52,862 [INFO]: Train Epoch: 4, 1180/2646 (45%), Loss: 6.586947\n",
            "INFO:20:Train Epoch: 4, 1180/2646 (45%), Loss: 6.586947\n",
            "2022-12-14 03:15:55,738 [INFO]: Train Epoch: 4, 1200/2646 (45%), Loss: 6.652475\n",
            "INFO:20:Train Epoch: 4, 1200/2646 (45%), Loss: 6.652475\n",
            "2022-12-14 03:15:58,409 [INFO]: Train Epoch: 4, 1220/2646 (46%), Loss: 6.500147\n",
            "INFO:20:Train Epoch: 4, 1220/2646 (46%), Loss: 6.500147\n",
            "2022-12-14 03:16:01,220 [INFO]: Train Epoch: 4, 1240/2646 (47%), Loss: 6.738439\n",
            "INFO:20:Train Epoch: 4, 1240/2646 (47%), Loss: 6.738439\n",
            "2022-12-14 03:16:03,915 [INFO]: Train Epoch: 4, 1260/2646 (48%), Loss: 6.710330\n",
            "INFO:20:Train Epoch: 4, 1260/2646 (48%), Loss: 6.710330\n",
            "2022-12-14 03:16:06,868 [INFO]: Train Epoch: 4, 1280/2646 (48%), Loss: 6.614091\n",
            "INFO:20:Train Epoch: 4, 1280/2646 (48%), Loss: 6.614091\n",
            "2022-12-14 03:16:09,671 [INFO]: Train Epoch: 4, 1300/2646 (49%), Loss: 6.634793\n",
            "INFO:20:Train Epoch: 4, 1300/2646 (49%), Loss: 6.634793\n",
            "2022-12-14 03:16:12,503 [INFO]: Train Epoch: 4, 1320/2646 (50%), Loss: 6.601214\n",
            "INFO:20:Train Epoch: 4, 1320/2646 (50%), Loss: 6.601214\n",
            "2022-12-14 03:16:15,336 [INFO]: Train Epoch: 4, 1340/2646 (51%), Loss: 6.613263\n",
            "INFO:20:Train Epoch: 4, 1340/2646 (51%), Loss: 6.613263\n",
            "2022-12-14 03:16:18,219 [INFO]: Train Epoch: 4, 1360/2646 (51%), Loss: 6.650311\n",
            "INFO:20:Train Epoch: 4, 1360/2646 (51%), Loss: 6.650311\n",
            "2022-12-14 03:16:20,939 [INFO]: Train Epoch: 4, 1380/2646 (52%), Loss: 6.545586\n",
            "INFO:20:Train Epoch: 4, 1380/2646 (52%), Loss: 6.545586\n",
            "2022-12-14 03:16:23,919 [INFO]: Train Epoch: 4, 1400/2646 (53%), Loss: 6.604304\n",
            "INFO:20:Train Epoch: 4, 1400/2646 (53%), Loss: 6.604304\n",
            "2022-12-14 03:16:26,742 [INFO]: Train Epoch: 4, 1420/2646 (54%), Loss: 6.629715\n",
            "INFO:20:Train Epoch: 4, 1420/2646 (54%), Loss: 6.629715\n",
            "2022-12-14 03:16:29,659 [INFO]: Train Epoch: 4, 1440/2646 (54%), Loss: 6.661994\n",
            "INFO:20:Train Epoch: 4, 1440/2646 (54%), Loss: 6.661994\n",
            "2022-12-14 03:16:32,462 [INFO]: Train Epoch: 4, 1460/2646 (55%), Loss: 6.827102\n",
            "INFO:20:Train Epoch: 4, 1460/2646 (55%), Loss: 6.827102\n",
            "2022-12-14 03:16:35,268 [INFO]: Train Epoch: 4, 1480/2646 (56%), Loss: 6.647479\n",
            "INFO:20:Train Epoch: 4, 1480/2646 (56%), Loss: 6.647479\n",
            "2022-12-14 03:16:38,103 [INFO]: Train Epoch: 4, 1500/2646 (57%), Loss: 6.689622\n",
            "INFO:20:Train Epoch: 4, 1500/2646 (57%), Loss: 6.689622\n",
            "2022-12-14 03:16:41,023 [INFO]: Train Epoch: 4, 1520/2646 (57%), Loss: 6.845515\n",
            "INFO:20:Train Epoch: 4, 1520/2646 (57%), Loss: 6.845515\n",
            "2022-12-14 03:16:43,932 [INFO]: Train Epoch: 4, 1540/2646 (58%), Loss: 6.546217\n",
            "INFO:20:Train Epoch: 4, 1540/2646 (58%), Loss: 6.546217\n",
            "2022-12-14 03:16:46,743 [INFO]: Train Epoch: 4, 1560/2646 (59%), Loss: 6.674479\n",
            "INFO:20:Train Epoch: 4, 1560/2646 (59%), Loss: 6.674479\n",
            "2022-12-14 03:16:49,697 [INFO]: Train Epoch: 4, 1580/2646 (60%), Loss: 6.668539\n",
            "INFO:20:Train Epoch: 4, 1580/2646 (60%), Loss: 6.668539\n",
            "2022-12-14 03:16:52,498 [INFO]: Train Epoch: 4, 1600/2646 (60%), Loss: 6.568241\n",
            "INFO:20:Train Epoch: 4, 1600/2646 (60%), Loss: 6.568241\n",
            "2022-12-14 03:16:55,309 [INFO]: Train Epoch: 4, 1620/2646 (61%), Loss: 6.665030\n",
            "INFO:20:Train Epoch: 4, 1620/2646 (61%), Loss: 6.665030\n",
            "2022-12-14 03:16:58,118 [INFO]: Train Epoch: 4, 1640/2646 (62%), Loss: 6.638363\n",
            "INFO:20:Train Epoch: 4, 1640/2646 (62%), Loss: 6.638363\n",
            "2022-12-14 03:17:01,015 [INFO]: Train Epoch: 4, 1660/2646 (63%), Loss: 6.610608\n",
            "INFO:20:Train Epoch: 4, 1660/2646 (63%), Loss: 6.610608\n",
            "2022-12-14 03:17:03,873 [INFO]: Train Epoch: 4, 1680/2646 (63%), Loss: 6.698799\n",
            "INFO:20:Train Epoch: 4, 1680/2646 (63%), Loss: 6.698799\n",
            "2022-12-14 03:17:06,605 [INFO]: Train Epoch: 4, 1700/2646 (64%), Loss: 6.605638\n",
            "INFO:20:Train Epoch: 4, 1700/2646 (64%), Loss: 6.605638\n",
            "2022-12-14 03:17:09,415 [INFO]: Train Epoch: 4, 1720/2646 (65%), Loss: 6.784522\n",
            "INFO:20:Train Epoch: 4, 1720/2646 (65%), Loss: 6.784522\n",
            "2022-12-14 03:17:12,372 [INFO]: Train Epoch: 4, 1740/2646 (66%), Loss: 6.581656\n",
            "INFO:20:Train Epoch: 4, 1740/2646 (66%), Loss: 6.581656\n",
            "2022-12-14 03:17:15,219 [INFO]: Train Epoch: 4, 1760/2646 (67%), Loss: 6.645391\n",
            "INFO:20:Train Epoch: 4, 1760/2646 (67%), Loss: 6.645391\n",
            "2022-12-14 03:17:18,177 [INFO]: Train Epoch: 4, 1780/2646 (67%), Loss: 6.556691\n",
            "INFO:20:Train Epoch: 4, 1780/2646 (67%), Loss: 6.556691\n",
            "2022-12-14 03:17:21,067 [INFO]: Train Epoch: 4, 1800/2646 (68%), Loss: 6.681421\n",
            "INFO:20:Train Epoch: 4, 1800/2646 (68%), Loss: 6.681421\n",
            "2022-12-14 03:17:23,834 [INFO]: Train Epoch: 4, 1820/2646 (69%), Loss: 6.696832\n",
            "INFO:20:Train Epoch: 4, 1820/2646 (69%), Loss: 6.696832\n",
            "2022-12-14 03:17:26,743 [INFO]: Train Epoch: 4, 1840/2646 (70%), Loss: 6.560767\n",
            "INFO:20:Train Epoch: 4, 1840/2646 (70%), Loss: 6.560767\n",
            "2022-12-14 03:17:29,521 [INFO]: Train Epoch: 4, 1860/2646 (70%), Loss: 6.605497\n",
            "INFO:20:Train Epoch: 4, 1860/2646 (70%), Loss: 6.605497\n",
            "2022-12-14 03:17:32,190 [INFO]: Train Epoch: 4, 1880/2646 (71%), Loss: 6.681121\n",
            "INFO:20:Train Epoch: 4, 1880/2646 (71%), Loss: 6.681121\n",
            "2022-12-14 03:17:34,862 [INFO]: Train Epoch: 4, 1900/2646 (72%), Loss: 6.634878\n",
            "INFO:20:Train Epoch: 4, 1900/2646 (72%), Loss: 6.634878\n",
            "2022-12-14 03:17:37,801 [INFO]: Train Epoch: 4, 1920/2646 (73%), Loss: 6.676305\n",
            "INFO:20:Train Epoch: 4, 1920/2646 (73%), Loss: 6.676305\n",
            "2022-12-14 03:17:40,558 [INFO]: Train Epoch: 4, 1940/2646 (73%), Loss: 6.659862\n",
            "INFO:20:Train Epoch: 4, 1940/2646 (73%), Loss: 6.659862\n",
            "2022-12-14 03:17:43,314 [INFO]: Train Epoch: 4, 1960/2646 (74%), Loss: 6.626070\n",
            "INFO:20:Train Epoch: 4, 1960/2646 (74%), Loss: 6.626070\n",
            "2022-12-14 03:17:46,335 [INFO]: Train Epoch: 4, 1980/2646 (75%), Loss: 6.678269\n",
            "INFO:20:Train Epoch: 4, 1980/2646 (75%), Loss: 6.678269\n",
            "2022-12-14 03:17:49,117 [INFO]: Train Epoch: 4, 2000/2646 (76%), Loss: 6.708590\n",
            "INFO:20:Train Epoch: 4, 2000/2646 (76%), Loss: 6.708590\n",
            "2022-12-14 03:17:51,939 [INFO]: Train Epoch: 4, 2020/2646 (76%), Loss: 6.743288\n",
            "INFO:20:Train Epoch: 4, 2020/2646 (76%), Loss: 6.743288\n",
            "2022-12-14 03:17:54,720 [INFO]: Train Epoch: 4, 2040/2646 (77%), Loss: 6.613690\n",
            "INFO:20:Train Epoch: 4, 2040/2646 (77%), Loss: 6.613690\n",
            "2022-12-14 03:17:57,659 [INFO]: Train Epoch: 4, 2060/2646 (78%), Loss: 6.573675\n",
            "INFO:20:Train Epoch: 4, 2060/2646 (78%), Loss: 6.573675\n",
            "2022-12-14 03:18:00,429 [INFO]: Train Epoch: 4, 2080/2646 (79%), Loss: 6.792822\n",
            "INFO:20:Train Epoch: 4, 2080/2646 (79%), Loss: 6.792822\n",
            "2022-12-14 03:18:03,192 [INFO]: Train Epoch: 4, 2100/2646 (79%), Loss: 6.711606\n",
            "INFO:20:Train Epoch: 4, 2100/2646 (79%), Loss: 6.711606\n",
            "2022-12-14 03:18:06,074 [INFO]: Train Epoch: 4, 2120/2646 (80%), Loss: 6.633029\n",
            "INFO:20:Train Epoch: 4, 2120/2646 (80%), Loss: 6.633029\n",
            "2022-12-14 03:18:09,096 [INFO]: Train Epoch: 4, 2140/2646 (81%), Loss: 6.629921\n",
            "INFO:20:Train Epoch: 4, 2140/2646 (81%), Loss: 6.629921\n",
            "2022-12-14 03:18:11,966 [INFO]: Train Epoch: 4, 2160/2646 (82%), Loss: 6.522440\n",
            "INFO:20:Train Epoch: 4, 2160/2646 (82%), Loss: 6.522440\n",
            "2022-12-14 03:18:14,777 [INFO]: Train Epoch: 4, 2180/2646 (82%), Loss: 6.692640\n",
            "INFO:20:Train Epoch: 4, 2180/2646 (82%), Loss: 6.692640\n",
            "2022-12-14 03:18:17,600 [INFO]: Train Epoch: 4, 2200/2646 (83%), Loss: 6.692556\n",
            "INFO:20:Train Epoch: 4, 2200/2646 (83%), Loss: 6.692556\n",
            "2022-12-14 03:18:20,347 [INFO]: Train Epoch: 4, 2220/2646 (84%), Loss: 6.721530\n",
            "INFO:20:Train Epoch: 4, 2220/2646 (84%), Loss: 6.721530\n",
            "2022-12-14 03:18:23,140 [INFO]: Train Epoch: 4, 2240/2646 (85%), Loss: 6.728557\n",
            "INFO:20:Train Epoch: 4, 2240/2646 (85%), Loss: 6.728557\n",
            "2022-12-14 03:18:25,925 [INFO]: Train Epoch: 4, 2260/2646 (85%), Loss: 6.625450\n",
            "INFO:20:Train Epoch: 4, 2260/2646 (85%), Loss: 6.625450\n",
            "2022-12-14 03:18:28,665 [INFO]: Train Epoch: 4, 2280/2646 (86%), Loss: 6.663313\n",
            "INFO:20:Train Epoch: 4, 2280/2646 (86%), Loss: 6.663313\n",
            "2022-12-14 03:18:31,572 [INFO]: Train Epoch: 4, 2300/2646 (87%), Loss: 6.584602\n",
            "INFO:20:Train Epoch: 4, 2300/2646 (87%), Loss: 6.584602\n",
            "2022-12-14 03:18:34,406 [INFO]: Train Epoch: 4, 2320/2646 (88%), Loss: 6.633882\n",
            "INFO:20:Train Epoch: 4, 2320/2646 (88%), Loss: 6.633882\n",
            "2022-12-14 03:18:37,061 [INFO]: Train Epoch: 4, 2340/2646 (88%), Loss: 6.572146\n",
            "INFO:20:Train Epoch: 4, 2340/2646 (88%), Loss: 6.572146\n",
            "2022-12-14 03:18:39,841 [INFO]: Train Epoch: 4, 2360/2646 (89%), Loss: 6.609895\n",
            "INFO:20:Train Epoch: 4, 2360/2646 (89%), Loss: 6.609895\n",
            "2022-12-14 03:18:42,646 [INFO]: Train Epoch: 4, 2380/2646 (90%), Loss: 6.601905\n",
            "INFO:20:Train Epoch: 4, 2380/2646 (90%), Loss: 6.601905\n",
            "2022-12-14 03:18:45,471 [INFO]: Train Epoch: 4, 2400/2646 (91%), Loss: 6.726731\n",
            "INFO:20:Train Epoch: 4, 2400/2646 (91%), Loss: 6.726731\n",
            "2022-12-14 03:18:48,506 [INFO]: Train Epoch: 4, 2420/2646 (91%), Loss: 6.651714\n",
            "INFO:20:Train Epoch: 4, 2420/2646 (91%), Loss: 6.651714\n",
            "2022-12-14 03:18:51,475 [INFO]: Train Epoch: 4, 2440/2646 (92%), Loss: 6.693265\n",
            "INFO:20:Train Epoch: 4, 2440/2646 (92%), Loss: 6.693265\n",
            "2022-12-14 03:18:54,288 [INFO]: Train Epoch: 4, 2460/2646 (93%), Loss: 6.712551\n",
            "INFO:20:Train Epoch: 4, 2460/2646 (93%), Loss: 6.712551\n",
            "2022-12-14 03:18:57,051 [INFO]: Train Epoch: 4, 2480/2646 (94%), Loss: 6.717600\n",
            "INFO:20:Train Epoch: 4, 2480/2646 (94%), Loss: 6.717600\n",
            "2022-12-14 03:18:59,908 [INFO]: Train Epoch: 4, 2500/2646 (94%), Loss: 6.673961\n",
            "INFO:20:Train Epoch: 4, 2500/2646 (94%), Loss: 6.673961\n",
            "2022-12-14 03:19:02,676 [INFO]: Train Epoch: 4, 2520/2646 (95%), Loss: 6.659475\n",
            "INFO:20:Train Epoch: 4, 2520/2646 (95%), Loss: 6.659475\n",
            "2022-12-14 03:19:05,597 [INFO]: Train Epoch: 4, 2540/2646 (96%), Loss: 6.623284\n",
            "INFO:20:Train Epoch: 4, 2540/2646 (96%), Loss: 6.623284\n",
            "2022-12-14 03:19:08,516 [INFO]: Train Epoch: 4, 2560/2646 (97%), Loss: 6.571658\n",
            "INFO:20:Train Epoch: 4, 2560/2646 (97%), Loss: 6.571658\n",
            "2022-12-14 03:19:11,350 [INFO]: Train Epoch: 4, 2580/2646 (98%), Loss: 6.622392\n",
            "INFO:20:Train Epoch: 4, 2580/2646 (98%), Loss: 6.622392\n",
            "2022-12-14 03:19:14,183 [INFO]: Train Epoch: 4, 2600/2646 (98%), Loss: 6.804119\n",
            "INFO:20:Train Epoch: 4, 2600/2646 (98%), Loss: 6.804119\n",
            "2022-12-14 03:19:16,971 [INFO]: Train Epoch: 4, 2620/2646 (99%), Loss: 6.749617\n",
            "INFO:20:Train Epoch: 4, 2620/2646 (99%), Loss: 6.749617\n",
            "2022-12-14 03:19:19,798 [INFO]: Train Epoch: 4, 2640/2646 (100%), Loss: 6.572893\n",
            "INFO:20:Train Epoch: 4, 2640/2646 (100%), Loss: 6.572893\n",
            "2022-12-14 03:19:20,336 [INFO]: Train Epoch: 4, total Loss: 17603.883826, mean Loss: 6.653017\n",
            "INFO:20:Train Epoch: 4, total Loss: 17603.883826, mean Loss: 6.653017\n",
            "2022-12-14 03:19:20,336 [DEBUG]: start validation\n",
            "DEBUG:10:start validation\n",
            "2022-12-14 03:19:22,918 [INFO]: Train Epoch: 4, validation loss is : 7.373\n",
            "INFO:20:Train Epoch: 4, validation loss is : 7.373\n",
            "2022-12-14 03:19:22,919 [DEBUG]: EarlyStopping counter:3 out of 10\n",
            "DEBUG:10:EarlyStopping counter:3 out of 10\n",
            "2022-12-14 03:19:22,919 [DEBUG]: #######: 5 ...\n",
            "DEBUG:10:#######: 5 ...\n",
            "2022-12-14 03:19:23,058 [INFO]: Train Epoch: 5, 0/2646 (0%), Loss: 6.676087\n",
            "INFO:20:Train Epoch: 5, 0/2646 (0%), Loss: 6.676087\n",
            "2022-12-14 03:19:25,747 [INFO]: Train Epoch: 5, 20/2646 (1%), Loss: 6.627806\n",
            "INFO:20:Train Epoch: 5, 20/2646 (1%), Loss: 6.627806\n",
            "2022-12-14 03:19:28,523 [INFO]: Train Epoch: 5, 40/2646 (2%), Loss: 6.637208\n",
            "INFO:20:Train Epoch: 5, 40/2646 (2%), Loss: 6.637208\n",
            "2022-12-14 03:19:31,251 [INFO]: Train Epoch: 5, 60/2646 (2%), Loss: 6.547258\n",
            "INFO:20:Train Epoch: 5, 60/2646 (2%), Loss: 6.547258\n",
            "2022-12-14 03:19:34,082 [INFO]: Train Epoch: 5, 80/2646 (3%), Loss: 6.757393\n",
            "INFO:20:Train Epoch: 5, 80/2646 (3%), Loss: 6.757393\n",
            "2022-12-14 03:19:36,830 [INFO]: Train Epoch: 5, 100/2646 (4%), Loss: 6.732958\n",
            "INFO:20:Train Epoch: 5, 100/2646 (4%), Loss: 6.732958\n",
            "2022-12-14 03:19:39,623 [INFO]: Train Epoch: 5, 120/2646 (5%), Loss: 6.664018\n",
            "INFO:20:Train Epoch: 5, 120/2646 (5%), Loss: 6.664018\n",
            "2022-12-14 03:19:42,339 [INFO]: Train Epoch: 5, 140/2646 (5%), Loss: 6.768357\n",
            "INFO:20:Train Epoch: 5, 140/2646 (5%), Loss: 6.768357\n",
            "2022-12-14 03:19:45,140 [INFO]: Train Epoch: 5, 160/2646 (6%), Loss: 6.673777\n",
            "INFO:20:Train Epoch: 5, 160/2646 (6%), Loss: 6.673777\n",
            "2022-12-14 03:19:47,796 [INFO]: Train Epoch: 5, 180/2646 (7%), Loss: 6.602758\n",
            "INFO:20:Train Epoch: 5, 180/2646 (7%), Loss: 6.602758\n",
            "2022-12-14 03:19:50,630 [INFO]: Train Epoch: 5, 200/2646 (8%), Loss: 6.647468\n",
            "INFO:20:Train Epoch: 5, 200/2646 (8%), Loss: 6.647468\n",
            "2022-12-14 03:19:53,595 [INFO]: Train Epoch: 5, 220/2646 (8%), Loss: 6.679363\n",
            "INFO:20:Train Epoch: 5, 220/2646 (8%), Loss: 6.679363\n",
            "2022-12-14 03:19:56,432 [INFO]: Train Epoch: 5, 240/2646 (9%), Loss: 6.654133\n",
            "INFO:20:Train Epoch: 5, 240/2646 (9%), Loss: 6.654133\n",
            "2022-12-14 03:19:59,176 [INFO]: Train Epoch: 5, 260/2646 (10%), Loss: 6.597560\n",
            "INFO:20:Train Epoch: 5, 260/2646 (10%), Loss: 6.597560\n",
            "2022-12-14 03:20:01,900 [INFO]: Train Epoch: 5, 280/2646 (11%), Loss: 6.585805\n",
            "INFO:20:Train Epoch: 5, 280/2646 (11%), Loss: 6.585805\n",
            "2022-12-14 03:20:04,664 [INFO]: Train Epoch: 5, 300/2646 (11%), Loss: 6.624092\n",
            "INFO:20:Train Epoch: 5, 300/2646 (11%), Loss: 6.624092\n",
            "2022-12-14 03:20:07,392 [INFO]: Train Epoch: 5, 320/2646 (12%), Loss: 6.753362\n",
            "INFO:20:Train Epoch: 5, 320/2646 (12%), Loss: 6.753362\n",
            "2022-12-14 03:20:10,198 [INFO]: Train Epoch: 5, 340/2646 (13%), Loss: 6.697295\n",
            "INFO:20:Train Epoch: 5, 340/2646 (13%), Loss: 6.697295\n",
            "2022-12-14 03:20:13,220 [INFO]: Train Epoch: 5, 360/2646 (14%), Loss: 6.575941\n",
            "INFO:20:Train Epoch: 5, 360/2646 (14%), Loss: 6.575941\n",
            "2022-12-14 03:20:16,040 [INFO]: Train Epoch: 5, 380/2646 (14%), Loss: 6.625105\n",
            "INFO:20:Train Epoch: 5, 380/2646 (14%), Loss: 6.625105\n",
            "2022-12-14 03:20:18,914 [INFO]: Train Epoch: 5, 400/2646 (15%), Loss: 6.717339\n",
            "INFO:20:Train Epoch: 5, 400/2646 (15%), Loss: 6.717339\n",
            "2022-12-14 03:20:21,813 [INFO]: Train Epoch: 5, 420/2646 (16%), Loss: 6.475496\n",
            "INFO:20:Train Epoch: 5, 420/2646 (16%), Loss: 6.475496\n",
            "2022-12-14 03:20:24,665 [INFO]: Train Epoch: 5, 440/2646 (17%), Loss: 6.628320\n",
            "INFO:20:Train Epoch: 5, 440/2646 (17%), Loss: 6.628320\n",
            "2022-12-14 03:20:27,408 [INFO]: Train Epoch: 5, 460/2646 (17%), Loss: 6.683346\n",
            "INFO:20:Train Epoch: 5, 460/2646 (17%), Loss: 6.683346\n",
            "2022-12-14 03:20:30,449 [INFO]: Train Epoch: 5, 480/2646 (18%), Loss: 6.656981\n",
            "INFO:20:Train Epoch: 5, 480/2646 (18%), Loss: 6.656981\n",
            "2022-12-14 03:20:33,324 [INFO]: Train Epoch: 5, 500/2646 (19%), Loss: 6.590353\n",
            "INFO:20:Train Epoch: 5, 500/2646 (19%), Loss: 6.590353\n",
            "2022-12-14 03:20:36,401 [INFO]: Train Epoch: 5, 520/2646 (20%), Loss: 6.569704\n",
            "INFO:20:Train Epoch: 5, 520/2646 (20%), Loss: 6.569704\n",
            "2022-12-14 03:20:39,355 [INFO]: Train Epoch: 5, 540/2646 (20%), Loss: 6.668827\n",
            "INFO:20:Train Epoch: 5, 540/2646 (20%), Loss: 6.668827\n",
            "2022-12-14 03:20:42,321 [INFO]: Train Epoch: 5, 560/2646 (21%), Loss: 6.648377\n",
            "INFO:20:Train Epoch: 5, 560/2646 (21%), Loss: 6.648377\n",
            "2022-12-14 03:20:45,302 [INFO]: Train Epoch: 5, 580/2646 (22%), Loss: 6.652747\n",
            "INFO:20:Train Epoch: 5, 580/2646 (22%), Loss: 6.652747\n",
            "2022-12-14 03:20:48,127 [INFO]: Train Epoch: 5, 600/2646 (23%), Loss: 6.585911\n",
            "INFO:20:Train Epoch: 5, 600/2646 (23%), Loss: 6.585911\n",
            "2022-12-14 03:20:50,996 [INFO]: Train Epoch: 5, 620/2646 (23%), Loss: 6.555056\n",
            "INFO:20:Train Epoch: 5, 620/2646 (23%), Loss: 6.555056\n",
            "2022-12-14 03:20:53,772 [INFO]: Train Epoch: 5, 640/2646 (24%), Loss: 6.551701\n",
            "INFO:20:Train Epoch: 5, 640/2646 (24%), Loss: 6.551701\n",
            "2022-12-14 03:20:56,604 [INFO]: Train Epoch: 5, 660/2646 (25%), Loss: 6.627571\n",
            "INFO:20:Train Epoch: 5, 660/2646 (25%), Loss: 6.627571\n",
            "2022-12-14 03:20:59,371 [INFO]: Train Epoch: 5, 680/2646 (26%), Loss: 6.739161\n",
            "INFO:20:Train Epoch: 5, 680/2646 (26%), Loss: 6.739161\n",
            "2022-12-14 03:21:02,118 [INFO]: Train Epoch: 5, 700/2646 (26%), Loss: 6.595762\n",
            "INFO:20:Train Epoch: 5, 700/2646 (26%), Loss: 6.595762\n",
            "2022-12-14 03:21:04,805 [INFO]: Train Epoch: 5, 720/2646 (27%), Loss: 6.666177\n",
            "INFO:20:Train Epoch: 5, 720/2646 (27%), Loss: 6.666177\n",
            "2022-12-14 03:21:07,642 [INFO]: Train Epoch: 5, 740/2646 (28%), Loss: 6.539144\n",
            "INFO:20:Train Epoch: 5, 740/2646 (28%), Loss: 6.539144\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 101, in <module>\n",
            "    trainer.train()\n",
            "  File \"/content/transformer/trainer/trainer.py\", line 41, in train\n",
            "    val_loss, metrics_score= self._train_epoch(epoch)\n",
            "  File \"/content/transformer/trainer/translate_trainer.py\", line 46, in _train_epoch\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\", line 197, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n"
      ],
      "metadata": {
        "id": "8lnxXGd7g-gS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370f3aab-fc46-4232-cb8b-91331eb0aad5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download zh_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW4LXRpKp325",
        "outputId": "cfc9b98a-1480-4652-c157-f9d47767ab72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 30.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting zh-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.4.0/zh_core_web_sm-3.4.0-py3-none-any.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 589 kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from zh-core-web-sm==3.4.0) (3.4.3)\n",
            "Collecting spacy-pkuseg<0.1.0,>=0.0.27\n",
            "  Downloading spacy_pkuseg-0.0.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.9)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.8)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.10.2)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.7.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (8.1.5)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.4.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.7)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (4.4.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->zh-core-web-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: spacy-pkuseg, zh-core-web-sm\n",
            "Successfully installed spacy-pkuseg-0.0.32 zh-core-web-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('zh_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python pre.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2mDEsNHqYTd",
        "outputId": "f295bf5e-457c-4d15-db82-b06b66195af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------loading-----------\n",
            " 57% 5663917/10000000 [39:22<1:18:56, 915.40it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QblyZ993qZ-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}